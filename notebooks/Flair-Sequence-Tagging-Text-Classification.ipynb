{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bdmVazyPKrMH"
   },
   "source": [
    "# Fine-Tune embeddings based on SPAM dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b9JpGbhNwG65"
   },
   "source": [
    "\n",
    "\n",
    "*   It comprises of popular and state-of-the-art word embeddings, such as GloVe, BERT, ELMo, Character Embeddings, etc. There are very easy to use thanks to the Flair API\n",
    "*   Flair’s interface allows us to combine different word embeddings and use them to embed documents. This in turn leads to a significant uptick in results\n",
    "\n",
    "*   ‘Flair Embedding’ is the signature embedding provided within the Flair library. It is powered by contextual string embeddings. \n",
    "*   It is also possible to fine-tune embeddings to your specific domain before using them for classification/tagging tasks!\n",
    "*   Flair supports a number of languages – and is always looking to add new ones\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R4nNjxK_bwt8"
   },
   "source": [
    "## Flair supports 3 main models as of the time of this notebook: TextClassifier, TextRegressor and SequenceTagger - ONLY RUNS ON COLLAB BECAUSE OF MODEL SIZES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 7805,
     "status": "ok",
     "timestamp": 1579374505468,
     "user": {
      "displayName": "George Gousios",
      "photoUrl": "",
      "userId": "00925390190798313342"
     },
     "user_tz": -60
    },
    "id": "mvYdWf6zBtM2",
    "outputId": "bda295de-bbf0-4ae3-c9df-38434f25cc3b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tiny-tokenizer in /usr/local/lib/python3.6/dist-packages (3.1.0)\n",
      "Requirement already satisfied: flair in /usr/local/lib/python3.6/dist-packages (0.4.4)\n",
      "Requirement already satisfied: gensim>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from flair) (3.6.0)\n",
      "Requirement already satisfied: ipython==7.6.1 in /usr/local/lib/python3.6/dist-packages (from flair) (7.6.1)\n",
      "Requirement already satisfied: segtok>=1.5.7 in /usr/local/lib/python3.6/dist-packages (from flair) (1.5.7)\n",
      "Requirement already satisfied: matplotlib>=2.2.3 in /usr/local/lib/python3.6/dist-packages (from flair) (3.1.2)\n",
      "Requirement already satisfied: ipython-genutils==0.2.0 in /usr/local/lib/python3.6/dist-packages (from flair) (0.2.0)\n",
      "Requirement already satisfied: bpemb>=0.2.9 in /usr/local/lib/python3.6/dist-packages (from flair) (0.3.0)\n",
      "Requirement already satisfied: transformers>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from flair) (2.3.0)\n",
      "Requirement already satisfied: sqlitedict>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from flair) (1.6.0)\n",
      "Requirement already satisfied: sklearn in /usr/local/lib/python3.6/dist-packages (from flair) (0.0)\n",
      "Requirement already satisfied: tabulate in /usr/local/lib/python3.6/dist-packages (from flair) (0.8.6)\n",
      "Requirement already satisfied: pytest>=3.6.4 in /usr/local/lib/python3.6/dist-packages (from flair) (3.6.4)\n",
      "Requirement already satisfied: deprecated>=1.2.4 in /usr/local/lib/python3.6/dist-packages (from flair) (1.2.7)\n",
      "Requirement already satisfied: langdetect in /usr/local/lib/python3.6/dist-packages (from flair) (1.0.7)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (from flair) (0.4.2)\n",
      "Requirement already satisfied: hyperopt>=0.1.1 in /usr/local/lib/python3.6/dist-packages (from flair) (0.1.2)\n",
      "Requirement already satisfied: pymongo in /usr/local/lib/python3.6/dist-packages (from flair) (3.10.0)\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from flair) (2019.12.20)\n",
      "Requirement already satisfied: mpld3==0.3 in /usr/local/lib/python3.6/dist-packages (from flair) (0.3)\n",
      "Requirement already satisfied: torch>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from flair) (1.3.1)\n",
      "Requirement already satisfied: tqdm>=4.26.0 in /usr/local/lib/python3.6/dist-packages (from flair) (4.28.1)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.20 in /usr/local/lib/python3.6/dist-packages (from flair) (1.24.3)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim>=3.4.0->flair) (1.4.1)\n",
      "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim>=3.4.0->flair) (1.9.0)\n",
      "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from gensim>=3.4.0->flair) (1.17.5)\n",
      "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim>=3.4.0->flair) (1.12.0)\n",
      "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython==7.6.1->flair) (4.7.0)\n",
      "Requirement already satisfied: backcall in /usr/local/lib/python3.6/dist-packages (from ipython==7.6.1->flair) (0.1.0)\n",
      "Requirement already satisfied: jedi>=0.10 in /usr/local/lib/python3.6/dist-packages (from ipython==7.6.1->flair) (0.15.2)\n",
      "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from ipython==7.6.1->flair) (2.1.3)\n",
      "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.6/dist-packages (from ipython==7.6.1->flair) (4.3.3)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from ipython==7.6.1->flair) (4.4.1)\n",
      "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython==7.6.1->flair) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit<2.1.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from ipython==7.6.1->flair) (2.0.10)\n",
      "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.6/dist-packages (from ipython==7.6.1->flair) (42.0.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->flair) (1.1.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->flair) (0.10.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->flair) (2.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->flair) (2.6.1)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from bpemb>=0.2.9->flair) (0.1.85)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from bpemb>=0.2.9->flair) (2.21.0)\n",
      "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers>=2.0.0->flair) (1.10.47)\n",
      "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers>=2.0.0->flair) (0.0.38)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sklearn->flair) (0.22.1)\n",
      "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest>=3.6.4->flair) (8.0.2)\n",
      "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest>=3.6.4->flair) (1.8.1)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest>=3.6.4->flair) (19.3.0)\n",
      "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.6/dist-packages (from pytest>=3.6.4->flair) (0.7.1)\n",
      "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.6/dist-packages (from pytest>=3.6.4->flair) (1.3.0)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.6/dist-packages (from deprecated>=1.2.4->flair) (1.11.2)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision->flair) (6.2.2)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from hyperopt>=0.1.1->flair) (0.16.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.6/dist-packages (from hyperopt>=0.1.1->flair) (2.4)\n",
      "Requirement already satisfied: boto>=2.32 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim>=3.4.0->flair) (2.49.0)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.6/dist-packages (from pexpect; sys_platform != \"win32\"->ipython==7.6.1->flair) (0.6.0)\n",
      "Requirement already satisfied: parso>=0.5.2 in /usr/local/lib/python3.6/dist-packages (from jedi>=0.10->ipython==7.6.1->flair) (0.5.2)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython==7.6.1->flair) (0.1.8)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->bpemb>=0.2.9->flair) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->bpemb>=0.2.9->flair) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->bpemb>=0.2.9->flair) (2019.11.28)\n",
      "Requirement already satisfied: botocore<1.14.0,>=1.13.47 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers>=2.0.0->flair) (1.13.47)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers>=2.0.0->flair) (0.9.4)\n",
      "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers>=2.0.0->flair) (0.2.1)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers>=2.0.0->flair) (7.0)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers>=2.0.0->flair) (0.14.1)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.47->boto3->transformers>=2.0.0->flair) (0.15.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install tiny-tokenizer flair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 65
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 15155,
     "status": "ok",
     "timestamp": 1579374529197,
     "user": {
      "displayName": "George Gousios",
      "photoUrl": "",
      "userId": "00925390190798313342"
     },
     "user_tz": -60
    },
    "id": "T7BosdqD4aKk",
    "outputId": "6fcfea9c-35e9-40c9-ea08-0372a3aeb110"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p style=\"color: red;\">\n",
       "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
       "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
       "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
       "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import io\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from flair.data import Corpus\n",
    "from flair.datasets import ClassificationCorpus, TREC_6              \n",
    "from typing import List\n",
    "\n",
    "from flair.data import Corpus\n",
    "from flair.datasets import TREC_6, WIKINER_ENGLISH\n",
    "\n",
    "\n",
    "from flair.data_fetcher import NLPTaskDataFetcher\n",
    "from flair.embeddings import WordEmbeddings, FlairEmbeddings, DocumentLSTMEmbeddings, CharacterEmbeddings \n",
    "from flair.embeddings import TokenEmbeddings, StackedEmbeddings, DocumentRNNEmbeddings, BertEmbeddings, OpenAIGPTEmbeddings\n",
    "from flair.models import TextClassifier, SequenceTagger\n",
    "from flair.visual.training_curves import Plotter\n",
    "from flair.trainers import ModelTrainer\n",
    "\n",
    "from flair.data import Dictionary\n",
    "from flair.models import LanguageModel\n",
    "from flair.trainers.language_model_trainer import LanguageModelTrainer, TextCorpus\n",
    "\n",
    "from google.colab import drive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3PWGk8iJBsMX"
   },
   "source": [
    "# Text Classification on Spam "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vL4AhUUeK2nC"
   },
   "source": [
    "### Flair expects FastText format of data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 317
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 46666,
     "status": "ok",
     "timestamp": 1579374583016,
     "user": {
      "displayName": "George Gousios",
      "photoUrl": "",
      "userId": "00925390190798313342"
     },
     "user_tz": -60
    },
    "id": "L49YKzkdC9WQ",
    "outputId": "562a42ff-49a4-4777-c985-c3b233eb340f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/gdrive\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>__label__ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>__label__ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>__label__spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>__label__ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>__label__ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           label                                               text\n",
       "0   __label__ham  Go until jurong point, crazy.. Available only ...\n",
       "1   __label__ham                      Ok lar... Joking wif u oni...\n",
       "2  __label__spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   __label__ham  U dun say so early hor... U c already then say...\n",
       "4   __label__ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drive.mount('/content/gdrive')\n",
    "datapath = 'gdrive/My Drive/spam.csv'                                           # spam.csv has to be in your google drive!\n",
    "df = pd.read_csv(datapath, encoding='latin-1')\n",
    "\n",
    "# from google.colab import files                                                # alternative to opening GDrive files - upload them manually\n",
    "# uploaded = files.upload()\n",
    "#df = pd.read_csv(io.StringIO(uploaded['spam.csv'].decode('latin-1')))\n",
    "\n",
    "df = df[['v1', 'v2']].rename(columns={\"v1\":\"label\", \"v2\":\"text\"})\n",
    "df['label'] = '__label__' + df['label'].astype(str)                             # That is how Flair expects the labels as of here:\n",
    "                                                                                # https://towardsdatascience.com/text-classification-with-state-of-the-art-nlp-library-flair-b541d7add21f\n",
    "                                                                                # a.k.a FastText format (Facebook)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hHUOdTnnC9e_"
   },
   "outputs": [],
   "source": [
    "## Split Train/Test/Validation\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "pathfiles = 'gdrive/My Drive/'\n",
    "data_folder = 'gdrive/My Drive/'\n",
    "\n",
    "\n",
    "df.iloc[0:int(len(df)*0.8)].to_csv('train.csv', sep='\\t', index = False, header = False)\n",
    "df.iloc[int(len(df)*0.8):int(len(df)*0.9)].to_csv('test.csv', sep='\\t', index = False, header = False)\n",
    "df.iloc[int(len(df)*0.9):].to_csv('dev.csv', sep='\\t', index = False, header = False);\n",
    "\n",
    "# train, test = train_test_split(df, test_size=0.2, random_state=1)\n",
    "# train, validation = train_test_split(train, test_size=0.2, random_state=1)\n",
    "\n",
    "# train.to_csv(pathfiles+'train.csv')\n",
    "# test.to_csv(pathfiles+'test.csv')\n",
    "# validation.to_csv(pathfiles+'dev.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 364
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2408,
     "status": "ok",
     "timestamp": 1579374593717,
     "user": {
      "displayName": "George Gousios",
      "photoUrl": "",
      "userId": "00925390190798313342"
     },
     "user_tz": -60
    },
    "id": "wWPo_Al37qaw",
    "outputId": "6f43ce1b-b6e2-4667-dc8a-6f58053b802b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-01-18 19:09:51,313 Reading data from .\n",
      "2020-01-18 19:09:51,316 Train: train.csv\n",
      "2020-01-18 19:09:51,319 Dev: dev.csv\n",
      "2020-01-18 19:09:51,320 Test: test.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated function (or staticmethod) load_classification_corpus. (Use 'flair.datasets' instead.) -- Deprecated since version 0.4.1.\n",
      "  after removing the cwd from sys.path.\n",
      "/usr/local/lib/python3.6/dist-packages/flair/data_fetcher.py:452: DeprecationWarning: Call to deprecated function (or staticmethod) read_text_classification_file. (Use 'flair.datasets' instead.) -- Deprecated since version 0.4.1.\n",
      "  train_file, tokenizer=tokenizer, max_tokens_per_doc=max_tokens_per_doc\n",
      "/usr/local/lib/python3.6/dist-packages/flair/data_fetcher.py:457: DeprecationWarning: Call to deprecated function (or staticmethod) read_text_classification_file. (Use 'flair.datasets' instead.) -- Deprecated since version 0.4.1.\n",
      "  test_file, tokenizer=tokenizer, max_tokens_per_doc=max_tokens_per_doc\n",
      "/usr/local/lib/python3.6/dist-packages/flair/data_fetcher.py:464: DeprecationWarning: Call to deprecated function (or staticmethod) read_text_classification_file. (Use 'flair.datasets' instead.) -- Deprecated since version 0.4.1.\n",
      "  dev_file, tokenizer=tokenizer, max_tokens_per_doc=max_tokens_per_doc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-01-18 19:09:53,305 Computing label dictionary. Progress:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4457/4457 [00:00<00:00, 339835.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-01-18 19:09:53,345 [b'ham', b'spam']\n",
      "Corpus: 4457 train + 558 dev + 557 test sentences\n",
      "<flair.data.Dictionary object at 0x7f8bcb000e80>\n",
      "defaultdict(<function Corpus.get_label_distribution.<locals>.<lambda> at 0x7f8bd04cc8c8>, {'ham': 3855, 'spam': 602})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#corpus: Corpus = NLPTaskDataFetcher.load_classification_corpus(Path(data_folder))           # old classification corpus load - depricated\n",
    "\n",
    "corpus = NLPTaskDataFetcher.load_classification_corpus(Path('./'), \n",
    "                                                       test_file='test.csv', \n",
    "                                                       dev_file='dev.csv', \n",
    "                                                       train_file='train.csv')               # WORKS for classification\n",
    "\n",
    "#corpus : Corpus = ClassificationCorpus(data_folder, test_file='test.csv', dev_file='dev.csv', train_file='train.csv')\n",
    "  \n",
    "# corpus: Corpus = TextCorpus(data_folder,                                                   # maybe used for fine tuning\n",
    "#                     dictionary=Dictionary,\n",
    "#                     #is_forward_lm=True,\n",
    "#                     character_level=True)\n",
    "\n",
    "label_dict = corpus.make_label_dictionary()\n",
    "\n",
    "print(corpus)                                                                                \n",
    "print(label_dict)                                                                            # Prints the different labels\n",
    "print(corpus.get_label_distribution())                                                       # label distribution in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 166484,
     "status": "ok",
     "timestamp": 1579374766236,
     "user": {
      "displayName": "George Gousios",
      "photoUrl": "",
      "userId": "00925390190798313342"
     },
     "user_tz": -60
    },
    "id": "LxiK8xr3XECl",
    "outputId": "21350b48-10be-4ece-93ff-5aebbb0686e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-01-18 19:10:00,252 https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings/glove.gensim.vectors.npy not found in cache, downloading to /tmp/tmpon4ywkqf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 160000128/160000128 [00:09<00:00, 17194814.96B/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-01-18 19:10:10,025 copying /tmp/tmpon4ywkqf to cache at /root/.flair/embeddings/glove.gensim.vectors.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-01-18 19:10:10,220 removing temp file /tmp/tmpon4ywkqf\n",
      "2020-01-18 19:10:11,326 https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings/glove.gensim not found in cache, downloading to /tmp/tmp17qwaxh4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21494764/21494764 [00:01<00:00, 11978502.29B/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-01-18 19:10:13,592 copying /tmp/tmp17qwaxh4 to cache at /root/.flair/embeddings/glove.gensim\n",
      "2020-01-18 19:10:13,612 removing temp file /tmp/tmp17qwaxh4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:402: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-01-18 19:10:15,887 https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings/lm-news-english-forward-1024-v0.2rc.pt not found in cache, downloading to /tmp/tmpw4qna2ud\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19689779/19689779 [00:01<00:00, 10467083.04B/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-01-18 19:10:18,312 copying /tmp/tmpw4qna2ud to cache at /root/.flair/embeddings/lm-news-english-forward-1024-v0.2rc.pt\n",
      "2020-01-18 19:10:18,331 removing temp file /tmp/tmpw4qna2ud\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-01-18 19:10:29,184 https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings/lm-news-english-backward-1024-v0.2rc.pt not found in cache, downloading to /tmp/tmp3vlhf4nm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19689779/19689779 [00:01<00:00, 11111307.98B/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-01-18 19:10:31,530 copying /tmp/tmp3vlhf4nm to cache at /root/.flair/embeddings/lm-news-english-backward-1024-v0.2rc.pt\n",
      "2020-01-18 19:10:31,551 removing temp file /tmp/tmp3vlhf4nm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-01-18 19:10:32,118 Computing label dictionary. Progress:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4457/4457 [00:00<00:00, 250630.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-01-18 19:10:32,139 [b'ham', b'spam']\n",
      "2020-01-18 19:10:32,146 ----------------------------------------------------------------------------------------------------\n",
      "2020-01-18 19:10:32,147 Model: \"TextClassifier(\n",
      "  (document_embeddings): DocumentRNNEmbeddings(\n",
      "    (embeddings): StackedEmbeddings(\n",
      "      (list_embedding_0): WordEmbeddings('glove')\n",
      "      (list_embedding_1): FlairEmbeddings(\n",
      "        (lm): LanguageModel(\n",
      "          (drop): Dropout(p=0.25, inplace=False)\n",
      "          (encoder): Embedding(275, 100)\n",
      "          (rnn): LSTM(100, 1024)\n",
      "          (decoder): Linear(in_features=1024, out_features=275, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (list_embedding_2): FlairEmbeddings(\n",
      "        (lm): LanguageModel(\n",
      "          (drop): Dropout(p=0.25, inplace=False)\n",
      "          (encoder): Embedding(275, 100)\n",
      "          (rnn): LSTM(100, 1024)\n",
      "          (decoder): Linear(in_features=1024, out_features=275, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (word_reprojection_map): Linear(in_features=2148, out_features=256, bias=True)\n",
      "    (rnn): GRU(256, 512, batch_first=True)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (decoder): Linear(in_features=512, out_features=2, bias=True)\n",
      "  (loss_function): CrossEntropyLoss()\n",
      ")\"\n",
      "2020-01-18 19:10:32,147 ----------------------------------------------------------------------------------------------------\n",
      "2020-01-18 19:10:32,148 Corpus: \"Corpus: 4457 train + 558 dev + 557 test sentences\"\n",
      "2020-01-18 19:10:32,149 ----------------------------------------------------------------------------------------------------\n",
      "2020-01-18 19:10:32,150 Parameters:\n",
      "2020-01-18 19:10:32,151  - learning_rate: \"0.1\"\n",
      "2020-01-18 19:10:32,152  - mini_batch_size: \"32\"\n",
      "2020-01-18 19:10:32,153  - patience: \"3\"\n",
      "2020-01-18 19:10:32,154  - anneal_factor: \"0.5\"\n",
      "2020-01-18 19:10:32,154  - max_epochs: \"10\"\n",
      "2020-01-18 19:10:32,155  - shuffle: \"True\"\n",
      "2020-01-18 19:10:32,156  - train_with_dev: \"False\"\n",
      "2020-01-18 19:10:32,157  - batch_growth_annealing: \"False\"\n",
      "2020-01-18 19:10:32,159 ----------------------------------------------------------------------------------------------------\n",
      "2020-01-18 19:10:32,160 Model training base path: \".\"\n",
      "2020-01-18 19:10:32,162 ----------------------------------------------------------------------------------------------------\n",
      "2020-01-18 19:10:32,163 Device: cuda:0\n",
      "2020-01-18 19:10:32,164 ----------------------------------------------------------------------------------------------------\n",
      "2020-01-18 19:10:32,166 Embeddings storage mode: cpu\n",
      "2020-01-18 19:10:32,168 ----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-01-18 19:10:32,488 epoch 1 - iter 0/140 - loss 0.77019781 - samples/sec: 1413.99\n",
      "2020-01-18 19:10:35,268 epoch 1 - iter 14/140 - loss 0.33843742 - samples/sec: 162.02\n",
      "2020-01-18 19:10:37,501 epoch 1 - iter 28/140 - loss 0.28109490 - samples/sec: 202.10\n",
      "2020-01-18 19:10:39,409 epoch 1 - iter 42/140 - loss 0.24306910 - samples/sec: 236.94\n",
      "2020-01-18 19:10:41,723 epoch 1 - iter 56/140 - loss 0.23098593 - samples/sec: 194.91\n",
      "2020-01-18 19:10:43,881 epoch 1 - iter 70/140 - loss 0.21363495 - samples/sec: 209.30\n",
      "2020-01-18 19:10:46,030 epoch 1 - iter 84/140 - loss 0.19400018 - samples/sec: 210.57\n",
      "2020-01-18 19:10:48,126 epoch 1 - iter 98/140 - loss 0.17833633 - samples/sec: 215.86\n",
      "2020-01-18 19:10:50,498 epoch 1 - iter 112/140 - loss 0.16816837 - samples/sec: 190.16\n",
      "2020-01-18 19:10:52,554 epoch 1 - iter 126/140 - loss 0.16221067 - samples/sec: 219.74\n",
      "2020-01-18 19:10:54,566 ----------------------------------------------------------------------------------------------------\n",
      "2020-01-18 19:10:54,567 EPOCH 1 done: loss 0.1563 - lr 0.1000\n",
      "2020-01-18 19:10:56,972 DEV : loss 0.08391359448432922 - score 0.9695\n",
      "2020-01-18 19:10:57,015 BAD EPOCHS (no improvement): 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type DocumentRNNEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type StackedEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type WordEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type FlairEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LanguageModel. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Embedding. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LSTM. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type GRU. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-01-18 19:11:00,040 ----------------------------------------------------------------------------------------------------\n",
      "2020-01-18 19:11:00,116 epoch 2 - iter 0/140 - loss 0.01854147 - samples/sec: 6255.28\n",
      "2020-01-18 19:11:00,945 epoch 2 - iter 14/140 - loss 0.09392258 - samples/sec: 550.44\n",
      "2020-01-18 19:11:01,760 epoch 2 - iter 28/140 - loss 0.10018810 - samples/sec: 560.94\n",
      "2020-01-18 19:11:02,572 epoch 2 - iter 42/140 - loss 0.08292114 - samples/sec: 563.27\n",
      "2020-01-18 19:11:03,463 epoch 2 - iter 56/140 - loss 0.07423747 - samples/sec: 518.36\n",
      "2020-01-18 19:11:04,298 epoch 2 - iter 70/140 - loss 0.07306584 - samples/sec: 549.96\n",
      "2020-01-18 19:11:05,107 epoch 2 - iter 84/140 - loss 0.07050647 - samples/sec: 565.29\n",
      "2020-01-18 19:11:05,910 epoch 2 - iter 98/140 - loss 0.07722138 - samples/sec: 570.50\n",
      "2020-01-18 19:11:06,688 epoch 2 - iter 112/140 - loss 0.07726955 - samples/sec: 588.79\n",
      "2020-01-18 19:11:07,535 epoch 2 - iter 126/140 - loss 0.08307600 - samples/sec: 539.59\n",
      "2020-01-18 19:11:08,301 ----------------------------------------------------------------------------------------------------\n",
      "2020-01-18 19:11:08,302 EPOCH 2 done: loss 0.0853 - lr 0.1000\n",
      "2020-01-18 19:11:09,093 DEV : loss 0.08923593908548355 - score 0.9731\n",
      "2020-01-18 19:11:09,134 BAD EPOCHS (no improvement): 0\n",
      "2020-01-18 19:11:12,024 ----------------------------------------------------------------------------------------------------\n",
      "2020-01-18 19:11:12,108 epoch 3 - iter 0/140 - loss 0.04661058 - samples/sec: 5970.16\n",
      "2020-01-18 19:11:12,949 epoch 3 - iter 14/140 - loss 0.10542297 - samples/sec: 544.22\n",
      "2020-01-18 19:11:13,795 epoch 3 - iter 28/140 - loss 0.07764189 - samples/sec: 544.94\n",
      "2020-01-18 19:11:14,649 epoch 3 - iter 42/140 - loss 0.08646266 - samples/sec: 534.72\n",
      "2020-01-18 19:11:15,452 epoch 3 - iter 56/140 - loss 0.07447636 - samples/sec: 569.52\n",
      "2020-01-18 19:11:16,226 epoch 3 - iter 70/140 - loss 0.08007467 - samples/sec: 592.03\n",
      "2020-01-18 19:11:17,058 epoch 3 - iter 84/140 - loss 0.07869946 - samples/sec: 548.70\n",
      "2020-01-18 19:11:17,901 epoch 3 - iter 98/140 - loss 0.07845797 - samples/sec: 542.61\n",
      "2020-01-18 19:11:18,684 epoch 3 - iter 112/140 - loss 0.07888190 - samples/sec: 586.40\n",
      "2020-01-18 19:11:19,492 epoch 3 - iter 126/140 - loss 0.07880402 - samples/sec: 565.62\n",
      "2020-01-18 19:11:20,278 ----------------------------------------------------------------------------------------------------\n",
      "2020-01-18 19:11:20,279 EPOCH 3 done: loss 0.0763 - lr 0.1000\n",
      "2020-01-18 19:11:21,089 DEV : loss 0.056725066155195236 - score 0.9821\n",
      "2020-01-18 19:11:21,129 BAD EPOCHS (no improvement): 0\n",
      "2020-01-18 19:11:23,879 ----------------------------------------------------------------------------------------------------\n",
      "2020-01-18 19:11:23,952 epoch 4 - iter 0/140 - loss 0.02908169 - samples/sec: 6319.21\n",
      "2020-01-18 19:11:24,797 epoch 4 - iter 14/140 - loss 0.04491831 - samples/sec: 539.98\n",
      "2020-01-18 19:11:25,581 epoch 4 - iter 28/140 - loss 0.04284200 - samples/sec: 584.11\n",
      "2020-01-18 19:11:26,428 epoch 4 - iter 42/140 - loss 0.05208814 - samples/sec: 539.47\n",
      "2020-01-18 19:11:27,311 epoch 4 - iter 56/140 - loss 0.05165238 - samples/sec: 518.04\n",
      "2020-01-18 19:11:28,170 epoch 4 - iter 70/140 - loss 0.05422600 - samples/sec: 531.77\n",
      "2020-01-18 19:11:28,955 epoch 4 - iter 84/140 - loss 0.05344413 - samples/sec: 582.79\n",
      "2020-01-18 19:11:29,765 epoch 4 - iter 98/140 - loss 0.05040386 - samples/sec: 569.21\n",
      "2020-01-18 19:11:30,574 epoch 4 - iter 112/140 - loss 0.05591566 - samples/sec: 565.19\n",
      "2020-01-18 19:11:31,415 epoch 4 - iter 126/140 - loss 0.05995367 - samples/sec: 543.76\n",
      "2020-01-18 19:11:32,144 ----------------------------------------------------------------------------------------------------\n",
      "2020-01-18 19:11:32,145 EPOCH 4 done: loss 0.0603 - lr 0.1000\n",
      "2020-01-18 19:11:32,946 DEV : loss 0.05234011635184288 - score 0.9857\n",
      "2020-01-18 19:11:32,988 BAD EPOCHS (no improvement): 0\n",
      "2020-01-18 19:11:35,818 ----------------------------------------------------------------------------------------------------\n",
      "2020-01-18 19:11:35,891 epoch 5 - iter 0/140 - loss 0.10797705 - samples/sec: 6210.31\n",
      "2020-01-18 19:11:36,761 epoch 5 - iter 14/140 - loss 0.08848703 - samples/sec: 524.14\n",
      "2020-01-18 19:11:37,587 epoch 5 - iter 28/140 - loss 0.07508683 - samples/sec: 553.18\n",
      "2020-01-18 19:11:38,390 epoch 5 - iter 42/140 - loss 0.06836610 - samples/sec: 569.66\n",
      "2020-01-18 19:11:39,266 epoch 5 - iter 56/140 - loss 0.06079619 - samples/sec: 522.53\n",
      "2020-01-18 19:11:40,135 epoch 5 - iter 70/140 - loss 0.05511623 - samples/sec: 525.19\n",
      "2020-01-18 19:11:40,895 epoch 5 - iter 84/140 - loss 0.05204953 - samples/sec: 602.77\n",
      "2020-01-18 19:11:41,730 epoch 5 - iter 98/140 - loss 0.05207281 - samples/sec: 546.45\n",
      "2020-01-18 19:11:42,501 epoch 5 - iter 112/140 - loss 0.04948873 - samples/sec: 594.16\n",
      "2020-01-18 19:11:43,397 epoch 5 - iter 126/140 - loss 0.05002518 - samples/sec: 509.51\n",
      "2020-01-18 19:11:44,233 ----------------------------------------------------------------------------------------------------\n",
      "2020-01-18 19:11:44,234 EPOCH 5 done: loss 0.0580 - lr 0.1000\n",
      "2020-01-18 19:11:45,041 DEV : loss 0.17434446513652802 - score 0.9444\n",
      "2020-01-18 19:11:45,082 BAD EPOCHS (no improvement): 1\n",
      "2020-01-18 19:11:45,085 ----------------------------------------------------------------------------------------------------\n",
      "2020-01-18 19:11:45,144 epoch 6 - iter 0/140 - loss 0.33057383 - samples/sec: 7679.30\n",
      "2020-01-18 19:11:45,997 epoch 6 - iter 14/140 - loss 0.08006943 - samples/sec: 533.67\n",
      "2020-01-18 19:11:47,264 epoch 6 - iter 28/140 - loss 0.06207106 - samples/sec: 359.83\n",
      "2020-01-18 19:11:48,053 epoch 6 - iter 42/140 - loss 0.06999841 - samples/sec: 578.62\n",
      "2020-01-18 19:11:48,826 epoch 6 - iter 56/140 - loss 0.06360224 - samples/sec: 592.19\n",
      "2020-01-18 19:11:49,658 epoch 6 - iter 70/140 - loss 0.05796209 - samples/sec: 549.99\n",
      "2020-01-18 19:11:50,514 epoch 6 - iter 84/140 - loss 0.05823733 - samples/sec: 534.11\n",
      "2020-01-18 19:11:51,390 epoch 6 - iter 98/140 - loss 0.05655442 - samples/sec: 521.67\n",
      "2020-01-18 19:11:52,266 epoch 6 - iter 112/140 - loss 0.05427804 - samples/sec: 521.29\n",
      "2020-01-18 19:11:53,079 epoch 6 - iter 126/140 - loss 0.05593375 - samples/sec: 562.33\n",
      "2020-01-18 19:11:53,851 ----------------------------------------------------------------------------------------------------\n",
      "2020-01-18 19:11:53,854 EPOCH 6 done: loss 0.0579 - lr 0.1000\n",
      "2020-01-18 19:11:54,654 DEV : loss 0.04647139087319374 - score 0.9875\n",
      "2020-01-18 19:11:54,703 BAD EPOCHS (no improvement): 0\n",
      "2020-01-18 19:11:57,512 ----------------------------------------------------------------------------------------------------\n",
      "2020-01-18 19:11:57,587 epoch 7 - iter 0/140 - loss 0.16388051 - samples/sec: 6123.63\n",
      "2020-01-18 19:11:58,449 epoch 7 - iter 14/140 - loss 0.04614694 - samples/sec: 528.52\n",
      "2020-01-18 19:11:59,332 epoch 7 - iter 28/140 - loss 0.06033169 - samples/sec: 516.71\n",
      "2020-01-18 19:12:00,198 epoch 7 - iter 42/140 - loss 0.05524883 - samples/sec: 526.74\n",
      "2020-01-18 19:12:00,966 epoch 7 - iter 56/140 - loss 0.05219421 - samples/sec: 595.69\n",
      "2020-01-18 19:12:01,795 epoch 7 - iter 70/140 - loss 0.05082115 - samples/sec: 551.91\n",
      "2020-01-18 19:12:02,612 epoch 7 - iter 84/140 - loss 0.04945431 - samples/sec: 561.76\n",
      "2020-01-18 19:12:03,483 epoch 7 - iter 98/140 - loss 0.05116276 - samples/sec: 525.16\n",
      "2020-01-18 19:12:04,272 epoch 7 - iter 112/140 - loss 0.04877472 - samples/sec: 579.21\n",
      "2020-01-18 19:12:05,151 epoch 7 - iter 126/140 - loss 0.05015151 - samples/sec: 518.51\n",
      "2020-01-18 19:12:05,901 ----------------------------------------------------------------------------------------------------\n",
      "2020-01-18 19:12:05,902 EPOCH 7 done: loss 0.0504 - lr 0.1000\n",
      "2020-01-18 19:12:06,724 DEV : loss 0.06799016147851944 - score 0.9767\n",
      "2020-01-18 19:12:06,764 BAD EPOCHS (no improvement): 1\n",
      "2020-01-18 19:12:06,765 ----------------------------------------------------------------------------------------------------\n",
      "2020-01-18 19:12:06,862 epoch 8 - iter 0/140 - loss 0.09255865 - samples/sec: 4796.34\n",
      "2020-01-18 19:12:07,749 epoch 8 - iter 14/140 - loss 0.06648397 - samples/sec: 513.71\n",
      "2020-01-18 19:12:08,586 epoch 8 - iter 28/140 - loss 0.04886852 - samples/sec: 547.71\n",
      "2020-01-18 19:12:09,382 epoch 8 - iter 42/140 - loss 0.04927214 - samples/sec: 574.69\n",
      "2020-01-18 19:12:10,306 epoch 8 - iter 56/140 - loss 0.05217053 - samples/sec: 493.36\n",
      "2020-01-18 19:12:11,115 epoch 8 - iter 70/140 - loss 0.04775273 - samples/sec: 565.55\n",
      "2020-01-18 19:12:11,942 epoch 8 - iter 84/140 - loss 0.05358667 - samples/sec: 553.76\n",
      "2020-01-18 19:12:12,889 epoch 8 - iter 98/140 - loss 0.04809063 - samples/sec: 481.74\n",
      "2020-01-18 19:12:13,720 epoch 8 - iter 112/140 - loss 0.04552205 - samples/sec: 549.58\n",
      "2020-01-18 19:12:14,528 epoch 8 - iter 126/140 - loss 0.04468659 - samples/sec: 564.60\n",
      "2020-01-18 19:12:15,262 ----------------------------------------------------------------------------------------------------\n",
      "2020-01-18 19:12:15,266 EPOCH 8 done: loss 0.0427 - lr 0.1000\n",
      "2020-01-18 19:12:16,082 DEV : loss 0.059854473918676376 - score 0.9767\n",
      "2020-01-18 19:12:16,131 BAD EPOCHS (no improvement): 2\n",
      "2020-01-18 19:12:16,132 ----------------------------------------------------------------------------------------------------\n",
      "2020-01-18 19:12:16,203 epoch 9 - iter 0/140 - loss 0.12352395 - samples/sec: 6398.39\n",
      "2020-01-18 19:12:17,059 epoch 9 - iter 14/140 - loss 0.06086148 - samples/sec: 532.12\n",
      "2020-01-18 19:12:17,899 epoch 9 - iter 28/140 - loss 0.05442697 - samples/sec: 543.88\n",
      "2020-01-18 19:12:18,740 epoch 9 - iter 42/140 - loss 0.04983701 - samples/sec: 543.79\n",
      "2020-01-18 19:12:19,506 epoch 9 - iter 56/140 - loss 0.04476036 - samples/sec: 597.77\n",
      "2020-01-18 19:12:20,371 epoch 9 - iter 70/140 - loss 0.04433312 - samples/sec: 527.58\n",
      "2020-01-18 19:12:21,236 epoch 9 - iter 84/140 - loss 0.04310837 - samples/sec: 527.14\n",
      "2020-01-18 19:12:22,044 epoch 9 - iter 98/140 - loss 0.04389976 - samples/sec: 565.90\n",
      "2020-01-18 19:12:22,845 epoch 9 - iter 112/140 - loss 0.04264164 - samples/sec: 570.76\n",
      "2020-01-18 19:12:23,670 epoch 9 - iter 126/140 - loss 0.04381760 - samples/sec: 554.49\n",
      "2020-01-18 19:12:24,490 ----------------------------------------------------------------------------------------------------\n",
      "2020-01-18 19:12:24,491 EPOCH 9 done: loss 0.0440 - lr 0.1000\n",
      "2020-01-18 19:12:25,322 DEV : loss 0.038847796618938446 - score 0.9875\n",
      "2020-01-18 19:12:25,365 BAD EPOCHS (no improvement): 3\n",
      "2020-01-18 19:12:28,177 ----------------------------------------------------------------------------------------------------\n",
      "2020-01-18 19:12:28,259 epoch 10 - iter 0/140 - loss 0.00711655 - samples/sec: 5784.55\n",
      "2020-01-18 19:12:29,177 epoch 10 - iter 14/140 - loss 0.02524745 - samples/sec: 496.91\n",
      "2020-01-18 19:12:30,005 epoch 10 - iter 28/140 - loss 0.01885830 - samples/sec: 554.29\n",
      "2020-01-18 19:12:30,855 epoch 10 - iter 42/140 - loss 0.03063161 - samples/sec: 537.82\n",
      "2020-01-18 19:12:31,666 epoch 10 - iter 56/140 - loss 0.02957407 - samples/sec: 563.93\n",
      "2020-01-18 19:12:32,507 epoch 10 - iter 70/140 - loss 0.03434113 - samples/sec: 543.56\n",
      "2020-01-18 19:12:33,355 epoch 10 - iter 84/140 - loss 0.03535004 - samples/sec: 538.30\n",
      "2020-01-18 19:12:34,186 epoch 10 - iter 98/140 - loss 0.03781254 - samples/sec: 550.99\n",
      "2020-01-18 19:12:35,056 epoch 10 - iter 112/140 - loss 0.04108369 - samples/sec: 525.78\n",
      "2020-01-18 19:12:35,926 epoch 10 - iter 126/140 - loss 0.04038713 - samples/sec: 524.44\n",
      "2020-01-18 19:12:36,669 ----------------------------------------------------------------------------------------------------\n",
      "2020-01-18 19:12:36,670 EPOCH 10 done: loss 0.0397 - lr 0.1000\n",
      "2020-01-18 19:12:37,483 DEV : loss 0.045074544847011566 - score 0.9875\n",
      "Epoch     9: reducing learning rate of group 0 to 5.0000e-02.\n",
      "2020-01-18 19:12:37,532 BAD EPOCHS (no improvement): 4\n",
      "2020-01-18 19:12:43,204 ----------------------------------------------------------------------------------------------------\n",
      "2020-01-18 19:12:43,205 Testing using best model ...\n",
      "2020-01-18 19:12:43,206 loading file best-model.pt\n",
      "2020-01-18 19:12:45,656 0.9803\t0.9803\t0.9803\n",
      "2020-01-18 19:12:45,657 \n",
      "MICRO_AVG: acc 0.9613 - f1-score 0.9803\n",
      "MACRO_AVG: acc 0.9155 - f1-score 0.9548000000000001\n",
      "ham        tp: 482 - fp: 9 - fn: 2 - tn: 64 - precision: 0.9817 - recall: 0.9959 - accuracy: 0.9777 - f1-score: 0.9887\n",
      "spam       tp: 64 - fp: 2 - fn: 9 - tn: 482 - precision: 0.9697 - recall: 0.8767 - accuracy: 0.8533 - f1-score: 0.9209\n",
      "2020-01-18 19:12:45,658 ----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'test_score': 0.9803,\n",
       " 'dev_score_history': [0.9695,\n",
       "  0.9731,\n",
       "  0.9821,\n",
       "  0.9857,\n",
       "  0.9444,\n",
       "  0.9875,\n",
       "  0.9767,\n",
       "  0.9767,\n",
       "  0.9875,\n",
       "  0.9875],\n",
       " 'train_loss_history': [0.1562643663824669,\n",
       "  0.08526597598434559,\n",
       "  0.07627157708629966,\n",
       "  0.060274979184448185,\n",
       "  0.05797219698184303,\n",
       "  0.057871729982019005,\n",
       "  0.05044327272501375,\n",
       "  0.042743298105363335,\n",
       "  0.04400325102532016,\n",
       "  0.03971474029655967],\n",
       " 'dev_loss_history': [tensor(0.0839, device='cuda:0'),\n",
       "  tensor(0.0892, device='cuda:0'),\n",
       "  tensor(0.0567, device='cuda:0'),\n",
       "  tensor(0.0523, device='cuda:0'),\n",
       "  tensor(0.1743, device='cuda:0'),\n",
       "  tensor(0.0465, device='cuda:0'),\n",
       "  tensor(0.0680, device='cuda:0'),\n",
       "  tensor(0.0599, device='cuda:0'),\n",
       "  tensor(0.0388, device='cuda:0'),\n",
       "  tensor(0.0451, device='cuda:0')]}"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#3. Choose your embedding types!\n",
    "word_embeddings = [WordEmbeddings('glove')\n",
    "                   ,FlairEmbeddings('news-forward-fast') \n",
    "                   ,FlairEmbeddings('news-backward-fast')\n",
    "                  #,OpenAIGPTEmbeddings()\n",
    "                   #,BertEmbeddings()\n",
    "                  ]\n",
    "\n",
    "#4. Init document embedding\n",
    "document_embeddings = DocumentRNNEmbeddings(word_embeddings, hidden_size=512, reproject_words=True, reproject_words_dimension=256)\n",
    "\n",
    "#5. Create the text classifier\n",
    "#classifier = TextClassifier(document_embeddings, label_dictionary=corpus.make_label_dictionary(), multi_label=False)\n",
    "classifier = TextClassifier(document_embeddings, label_dictionary=corpus.make_label_dictionary(), multi_label=False)    # Try with multi_label = True\n",
    "\n",
    "\n",
    "#6. Initialize the text classifier trainer\n",
    "#trainer = TextClassifier(classifier, corpus, label_dict)\n",
    "\n",
    "classifier = TextClassifier(document_embeddings, label_dict, multi_label=False)\n",
    "\n",
    "trainer = ModelTrainer(classifier, corpus)\n",
    "\n",
    "#7. Start the training\n",
    "trainer.train('./' \n",
    "              #, learning_rate=0.1\n",
    "              #, mini_batch_size=32\n",
    "              #, anneal_factor=0.5\n",
    "              #, patience=5\n",
    "              , max_epochs=10)\n",
    "\n",
    "# #8. Predict something\n",
    "# sentence = classifier.predict(Sentence('hello'))\n",
    "# print(sentence.labels)\n",
    "\n",
    "#9. plot training curves (optional)                                             # need to locate files on disk to work!\n",
    "# plotter = Plotter()\n",
    "# plotter.plot_training_curves('./resources/sentiment_classifier-11classes-en/results/loss.tsv')\n",
    "# plotter.plot_weights('./resources/sentiment_classifier-11classes-en/results/weights.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "43dv3thz45sq"
   },
   "source": [
    "# Text Classification on TREC_6 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 395
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2515,
     "status": "error",
     "timestamp": 1563462614664,
     "user": {
      "displayName": "George Gousios",
      "photoUrl": "",
      "userId": "00925390190798313342"
     },
     "user_tz": -120
    },
    "id": "UuOdwg-yKrM7",
    "outputId": "9ef91cbf-ee88-422b-86c6-c378418226ec"
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-d02d0980bd83>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcorpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTREC_6\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# 2. create the label dictionary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mlabel_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_label_dictionary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/flair/datasets.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, base_path, in_memory)\u001b[0m\n\u001b[1;32m   1044\u001b[0m             cached_path(\n\u001b[1;32m   1045\u001b[0m                 \u001b[0;34mf\"{trec_path}{original_filename}\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1046\u001b[0;31m                 \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"datasets\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mdataset_name\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m\"original\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1047\u001b[0m             )\n\u001b[1;32m   1048\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/flair/file_utils.py\u001b[0m in \u001b[0;36mcached_path\u001b[0;34m(url_or_filename, cache_dir)\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mparsed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscheme\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"http\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"https\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;31m# URL, so get it from the cache (downloading if necessary)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mget_from_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl_or_filename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_cache\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mparsed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscheme\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"\"\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl_or_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;31m# File, and it exists.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/flair/file_utils.py\u001b[0m in \u001b[0;36mget_from_cache\u001b[0;34m(url, cache_dir)\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m         raise IOError(\n\u001b[0;32m--> 164\u001b[0;31m             \u001b[0;34mf\"HEAD request failed for url {url} with status code {response.status_code}.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m         )\n\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: HEAD request failed for url http://cogcomp.org/Data/QA/QC/train_5500.label with status code 301."
     ]
    }
   ],
   "source": [
    "# 1. get the corpus\n",
    "\n",
    "corpus = TREC_6()\n",
    "\n",
    "# 2. create the label dictionary\n",
    "label_dict = corpus.make_label_dictionary()\n",
    "\n",
    "print(len(corpus.train))\n",
    "\n",
    "# 3. make a list of word embeddings\n",
    "word_embeddings = [#WordEmbeddings('glove')\n",
    "                    #,FlairEmbeddings('news-forward')                           # unknown CUDA error when trying to load these - It's because of GPU VRAM\n",
    "                    #,FlairEmbeddings('news-forward-fast')                         \n",
    "                    #,FlairEmbeddings('news-backward')          \n",
    "                    #,FlairEmbeddings('multi-forward')\n",
    "                    #,FlairEmbeddings('multi-backward')\n",
    "                    BertEmbeddings('bert-base-multilingual-uncased')            # BERT + GPT embeddings = really slow training!\n",
    "                    ,OpenAIGPTEmbeddings()\n",
    "                    #,BytePairEmbeddings(language='en')                         # new in Flair, supposedly very good for small models\n",
    "                    #,CharacterEmbeddings()                     \n",
    "                  ]\n",
    "\n",
    "# 4. initialize document embedding by passing list of word embeddings\n",
    "# Can choose between many RNN types (GRU by default, to change use rnn_type parameter)\n",
    "document_embeddings: DocumentRNNEmbeddings = DocumentRNNEmbeddings(word_embeddings\n",
    "                                                                     , hidden_size=512\n",
    "                                                                     , reproject_words=True\n",
    "                                                                     , reproject_words_dimension=256\n",
    "                                                                     #, rnn_type='LSTM'\n",
    "                                                                     )\n",
    "\n",
    "# 5. create the text classifier\n",
    "classifier = TextClassifier(document_embeddings, label_dictionary=label_dict)\n",
    "\n",
    "# 6. initialize the text classifier trainer\n",
    "trainer = ModelTrainer(classifier, corpus)\n",
    "\n",
    "# 7. start the training\n",
    "trainer.train('resources/taggers/ag_news',                                      \n",
    "              learning_rate=0.1,\n",
    "              mini_batch_size=32,\n",
    "              anneal_factor=0.5,\n",
    "              patience=5,\n",
    "              max_epochs=15,\n",
    "             checkpoint=True)                                                                # checkpoint set to True enables you to stop training and resume later!\n",
    "\n",
    "# checkpoint = tagger.load_checkpoint(Path('resources/taggers/example-ner/checkpoint.pt'))   # Resume from Checkpoint if previously paused training\n",
    "# trainer = ModelTrainer.load_from_checkpoint(checkpoint, corpus)\n",
    "# trainer.train('resources/taggers/example-ner',\n",
    "#               EvaluationMetric.MICRO_F1_SCORE,\n",
    "#               learning_rate=0.1,\n",
    "#               mini_batch_size=32,\n",
    "#               anneal_factor=0.5,\n",
    "#               max_epochs=15,\n",
    "#               checkpoint=True)\n",
    "\n",
    "# 8. plot training curves (optional)                                                         # need to locate files on disk to work!\n",
    "# plotter = Plotter()\n",
    "# plotter.plot_training_curves('loss.tsv')\n",
    "# plotter.plot_weights('weights.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "81H067dQKrM_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AyaN45heDNgf"
   },
   "source": [
    "# NER on custom dataset (Kaggle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 504
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2145,
     "status": "error",
     "timestamp": 1579374796554,
     "user": {
      "displayName": "George Gousios",
      "photoUrl": "",
      "userId": "00925390190798313342"
     },
     "user_tz": -60
    },
    "id": "sQgOYSG6chVD",
    "outputId": "3c8e5dee-911d-4ffa-d7ff-6cc650477953"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
      "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-ddd634b8512e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/gdrive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mdatapath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'gdrive/My Drive/ner_dataset.csv'\u001b[0m                                    \u001b[0;31m# train_ner.csv has to be in your google drive!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatapath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'latin-1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"ffill\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# split and save train/test/dev to csvs because flair only reads shit from disk when loading corpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    683\u001b[0m         )\n\u001b[1;32m    684\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 685\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1135\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1136\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1915\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1917\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1918\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File b'gdrive/My Drive/ner_dataset.csv' does not exist: b'gdrive/My Drive/ner_dataset.csv'"
     ]
    }
   ],
   "source": [
    "# IMPORTANT: Data is expected to be of 'Obama N B-PER' (word pos ner) format, WITHOUT full stop lines, and a gap line separating different sentences\n",
    "\n",
    "# Load data csv\n",
    "\n",
    "drive.mount('/content/gdrive')\n",
    "os.path.exists('gdrive/My Drive/ner_dataset.csv')\n",
    "\n",
    "# from google.colab import files                                                # alternatively upload it manually using this snippet\n",
    "# uploaded = files.upload()\n",
    "# df = pd.read_csv(io.StringIO(uploaded['train_ner.csv']\n",
    "#                              .decode('latin-1').fillna(method=\"ffill\")))\n",
    "\n",
    "drive.mount('/content/gdrive')\n",
    "datapath = 'gdrive/My Drive/ner_dataset.csv'                                    # train_ner.csv has to be in your google drive!\n",
    "df = pd.read_csv(datapath, encoding='latin-1').fillna(method=\"ffill\")\n",
    "\n",
    "# split and save train/test/dev to csvs because flair only reads shit from disk when loading corpus\n",
    "\n",
    "df = df[['Word', 'POS', 'Tag']]                                                 # discard sentence no\n",
    "\n",
    "df.rename(columns={'Word': 'text', 'POS': 'pos', 'Tag':'ner'}, inplace=True)    # rename it as flair expects\n",
    "\n",
    "data_folder = 'gdrive/My Drive/nerReformed'\n",
    "\n",
    "### replace rows where text=='.' with a blank row (GH proposal)\n",
    "\n",
    "df.text = df.text.replace('.', '')\n",
    "df.pos = df.pos.replace('.', '')\n",
    "df.ner = df.pos.replace('', '')\n",
    "\n",
    "#df.loc[(df['pos']=='.')]                 # should return nothing\n",
    "\n",
    "if not os.path.exists(data_folder):\n",
    "  os.mkdir(data_folder)\n",
    "  \n",
    "# ensure that a single sentence is not spitted in different train/dev/test sets\n",
    "\n",
    "\n",
    "\n",
    "# df.iloc[0:int(len(df)*0.8)].to_csv(data_folder+'/train.csv', sep='\\t', index = False, header = False)\n",
    "# df.iloc[int(len(df)*0.8):int(len(df)*0.9)].to_csv(data_folder+'/test.csv', sep='\\t', index = False, header = False)\n",
    "# df.iloc[int(len(df)*0.9):].to_csv(data_folder+'/dev.csv', sep='\\t', index = False, header = False);\n",
    "\n",
    "df.head(25)          # check if data is as we want it to be\n",
    "\n",
    "### For Flair ColumnCorpus we need this format:\n",
    "\n",
    "# - George N B-PER\n",
    "# - Washington N I-PER\n",
    "# - went V O\n",
    "# - to P O\n",
    "# - Washington N B-LOC\n",
    "# - ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 362
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1656,
     "status": "ok",
     "timestamp": 1563817048419,
     "user": {
      "displayName": "George Gousios",
      "photoUrl": "",
      "userId": "00925390190798313342"
     },
     "user_tz": -120
    },
    "id": "MopVNbKVnyDb",
    "outputId": "50f72792-ed58-4c80-81ee-f3e0fd137a91"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainstart: 0\n",
      "Trainstop: 838861\n",
      "Teststart: 838862\n",
      "943742\n",
      "Teststop: 943742\n",
      "          text  pos  ner\n",
      "943737  linked  VBN  VBN\n",
      "943738      to   TO   TO\n",
      "943739     the   DT   DT\n",
      "943740  Juarez  NNP  NNP\n",
      "943741  cartel   NN   NN\n",
      "Devstart: 943743\n",
      "Devstop: 1048574\n",
      "              text  pos  ner\n",
      "1048570       they  PRP  PRP\n",
      "1048571  responded  VBD  VBD\n",
      "1048572         to   TO   TO\n",
      "1048573        the   DT   DT\n",
      "1048574     attack   NN   NN\n"
     ]
    }
   ],
   "source": [
    "trainstart = df.index[0]\n",
    "print(\"Trainstart: \"+str(trainstart))                                           # should return 0 \n",
    "tmpdf = df[int(len(df)*0.8):]                                                   \n",
    "trainstop = tmpdf.loc[0:tmpdf['text'].gt('').idxmin(),:].index[-1]              # from 0.8*df to next blank => end of sentence!\n",
    "print(\"Trainstop: \"+str(trainstop))                                             # should return 838861 (it's where the last train sentence ends)\n",
    "train = df.iloc[0:trainstop]\n",
    "#print(train.tail())\n",
    "\n",
    "\n",
    "teststart = trainstop+1\n",
    "#tmpdf = df.iloc[teststart:int(len(df)),]\n",
    "tmpdf = df[int(len(df)*0.9):]\n",
    "print(\"Teststart: \"+str(int(teststart)))\n",
    "print(tmpdf['text'].gt('').idxmin())\n",
    "teststop = tmpdf.loc[int(len(df)*0.9):tmpdf['text'].gt('').idxmin(),:].index[-1]\n",
    "print(\"Teststop: \"+str(teststop))\n",
    "test = df.iloc[teststart:teststop]\n",
    "print(test.tail())\n",
    "\n",
    "devstart = teststop+1\n",
    "print(\"Devstart: \"+str(devstart))\n",
    "dev = df.iloc[devstart:]\n",
    "print(\"Devstop: \"+str(dev.index[-1]))\n",
    "print(dev.tail())\n",
    "\n",
    "\n",
    "train.to_csv(data_folder+'/train.csv', sep='\\t', index = False, header = False)\n",
    "test.to_csv(data_folder+'/test.csv', sep='\\t', index = False, header = False)\n",
    "dev.to_csv(data_folder+'/dev.csv', sep='\\t', index = False, header = False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 90
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 16436,
     "status": "ok",
     "timestamp": 1563817068187,
     "user": {
      "displayName": "George Gousios",
      "photoUrl": "",
      "userId": "00925390190798313342"
     },
     "user_tz": -120
    },
    "id": "Gd779z-lk-qL",
    "outputId": "c4374dc6-8e9b-4896-da59-b8f828e2c4b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-07-22 17:37:07,954 Reading data from gdrive/My Drive/nerReformed\n",
      "2019-07-22 17:37:07,956 Train: gdrive/My Drive/nerReformed/train.csv\n",
      "2019-07-22 17:37:07,963 Dev: gdrive/My Drive/nerReformed/dev.csv\n",
      "2019-07-22 17:37:07,964 Test: gdrive/My Drive/nerReformed/test.csv\n"
     ]
    }
   ],
   "source": [
    "from flair.datasets import ColumnCorpus\n",
    "\n",
    "# define columns\n",
    "\n",
    "columns = {0: 'text', 1: 'pos', 2: 'ner'}\n",
    "tag_type = 'ner'\n",
    "\n",
    "# 1. Load Corpus\n",
    "\n",
    "# load corpus by pointing to folder. Train, dev and test gets identified automatically. \n",
    "corpus: Corpus = ColumnCorpus(data_folder, columns,\n",
    "                              train_file='train.csv',\n",
    "                              test_file='test.csv',\n",
    "                              dev_file='dev.csv')                               # out of memory if no downsampling done! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 16028,
     "status": "ok",
     "timestamp": 1563817068191,
     "user": {
      "displayName": "George Gousios",
      "photoUrl": "",
      "userId": "00925390190798313342"
     },
     "user_tz": -120
    },
    "id": "pz1V1pNzqfxE",
    "outputId": "38520861-1bfa-4352-b32e-2e503af13076"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train has a size of 38188 sentences\n"
     ]
    }
   ],
   "source": [
    "print(\"Train has a size of \"+str(len(corpus.train))+\" sentences\")             # Flair thinks each split is only a sentence!\n",
    "#print(corpus.train[0][1])                                                     # list of sentences, so [][] is token/word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 92
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 17022,
     "status": "ok",
     "timestamp": 1563817069436,
     "user": {
      "displayName": "George Gousios",
      "photoUrl": "",
      "userId": "00925390190798313342"
     },
     "user_tz": -120
    },
    "id": "3tqwRsFVpAzo",
    "outputId": "a7d32163-fcd6-4835-8230-59014856cfc9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus: 38188 train + 4815 dev + 4757 test sentences\n",
      "['<unk>', 'O', 'NNS', 'IN', 'VBP', 'VBN', 'NNP', 'TO', 'VB', 'DT', 'NN', 'CC', 'JJ', 'VBD', 'WP', '``', 'CD', 'PRP', 'VBZ', 'POS', 'VBG', 'RB', ',', 'WRB', 'PRP$', 'MD', 'WDT', 'JJR', ':', 'JJS', 'WP$', 'RP', 'PDT', 'NNPS', 'EX', 'RBS', 'LRB', 'RRB', '$', 'RBR', ';', '', 'UH', 'FW', '<START>', '<STOP>']\n",
      "defaultdict(<function Corpus.get_label_distribution.<locals>.<lambda> at 0x7fe41898f9d8>, {})\n"
     ]
    }
   ],
   "source": [
    "tag_dictionary = corpus.make_tag_dictionary('ner')\n",
    "\n",
    "print(corpus)                                                                                \n",
    "print(tag_dictionary.get_items())                                                                          # Prints the different labels\n",
    "print(corpus.get_label_distribution())                                                       # label distribution in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 746
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3400320,
     "status": "ok",
     "timestamp": 1563807198146,
     "user": {
      "displayName": "George Gousios",
      "photoUrl": "",
      "userId": "00925390190798313342"
     },
     "user_tz": -120
    },
    "id": "WcOTDAnWDQGO",
    "outputId": "a7fcd01c-97e9-49d0-f4fe-8bd962b802a2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-07-22 18:04:39,425 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-22 18:04:39,426 Evaluation method: MICRO_F1_SCORE\n",
      "2019-07-22 18:04:39,971 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-22 18:04:41,664 epoch 1 - iter 0/597 - loss 90.36824036\n",
      "2019-07-22 18:05:37,665 epoch 1 - iter 59/597 - loss 45.03847485\n",
      "2019-07-22 18:06:26,436 epoch 1 - iter 118/597 - loss 31.50181136\n",
      "2019-07-22 18:07:19,312 epoch 1 - iter 177/597 - loss 24.92577699\n",
      "2019-07-22 18:08:07,531 epoch 1 - iter 236/597 - loss 21.07376778\n",
      "2019-07-22 18:09:01,452 epoch 1 - iter 295/597 - loss 18.43487070\n",
      "2019-07-22 18:09:50,972 epoch 1 - iter 354/597 - loss 16.46917385\n",
      "2019-07-22 18:10:45,472 epoch 1 - iter 413/597 - loss 14.93163178\n",
      "2019-07-22 18:11:34,911 epoch 1 - iter 472/597 - loss 13.70888507\n",
      "2019-07-22 18:12:28,972 epoch 1 - iter 531/597 - loss 12.72079313\n",
      "2019-07-22 18:13:18,820 epoch 1 - iter 590/597 - loss 11.89781154\n",
      "2019-07-22 18:13:24,600 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-22 18:13:24,602 EPOCH 1 done: loss 11.8216 - lr 0.1000 - bad epochs 0\n",
      "2019-07-22 18:14:49,151 DEV : loss 1.9325082302093506 - score 0.9729\n",
      "2019-07-22 18:16:13,472 TEST : loss 2.0305705070495605 - score 0.972\n",
      "2019-07-22 18:16:18,039 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-22 18:16:20,393 epoch 2 - iter 0/597 - loss 2.35211706\n",
      "2019-07-22 18:17:14,347 epoch 2 - iter 59/597 - loss 4.35866121\n",
      "2019-07-22 18:18:12,416 epoch 2 - iter 118/597 - loss 4.39987183\n",
      "2019-07-22 18:19:02,581 epoch 2 - iter 177/597 - loss 4.40425858\n",
      "2019-07-22 18:19:57,569 epoch 2 - iter 236/597 - loss 4.36185337\n",
      "2019-07-22 18:20:46,205 epoch 2 - iter 295/597 - loss 4.22996952\n",
      "2019-07-22 18:21:40,916 epoch 2 - iter 354/597 - loss 4.21525964\n",
      "2019-07-22 18:22:31,639 epoch 2 - iter 413/597 - loss 4.13039745\n",
      "2019-07-22 18:23:25,120 epoch 2 - iter 472/597 - loss 4.05098680\n",
      "2019-07-22 18:24:15,016 epoch 2 - iter 531/597 - loss 3.99764493\n",
      "2019-07-22 18:25:08,847 epoch 2 - iter 590/597 - loss 3.95138110\n",
      "2019-07-22 18:25:14,136 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-22 18:25:14,138 EPOCH 2 done: loss 3.9399 - lr 0.1000 - bad epochs 0\n",
      "2019-07-22 18:26:33,424 DEV : loss 1.4842556715011597 - score 0.9783\n",
      "2019-07-22 18:27:57,946 TEST : loss 1.535582184791565 - score 0.9781\n",
      "2019-07-22 18:28:02,497 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-22 18:28:05,162 epoch 3 - iter 0/597 - loss 5.30321693\n",
      "2019-07-22 18:28:58,773 epoch 3 - iter 59/597 - loss 3.50479158\n"
     ]
    }
   ],
   "source": [
    "# ALSO: in_memory=False  :    using PyTorch dataloaders, does not keep whole dataset in memory - can support much larger datasets\n",
    "\n",
    "# 2. Create Tag dictionary\n",
    "\n",
    "tag_dictionary = corpus.make_tag_dictionary('ner')\n",
    "\n",
    "# 3. Embeddings\n",
    "embedding_types: List[TokenEmbeddings] = [\n",
    "    WordEmbeddings('glove'),                                                    # Recommended by Flair paper: glove + bi-directional Flair\n",
    "    FlairEmbeddings('news-forward'),\n",
    "    FlairEmbeddings('news-backward')\n",
    "    # OpenAIGPTEmbeddings(),\n",
    "    # BertEmbeddings()                                                          # BERT embeddings = VERY slow training (30 mins for first iteration!)\n",
    "    # CharacterEmbeddings(),\n",
    "    # FlairEmbeddings('multi-forward',\n",
    "    # FlairEmbeddings('multi-backward')\n",
    "    # BertEmbeddings('bert-base-multilingual-uncased'\n",
    "    # BytePairEmbeddings(language='en')                                         # new in Flair, supposedly very good for small models\n",
    "]\n",
    "\n",
    "embeddings: StackedEmbeddings = StackedEmbeddings(embeddings=embedding_types)\n",
    "\n",
    "# 4. Initialize sequence tagger\n",
    "tagger: SequenceTagger = SequenceTagger(hidden_size=256,\n",
    "                                        embeddings=embeddings,\n",
    "                                        tag_dictionary=tag_dictionary,\n",
    "                                        tag_type=tag_type,\n",
    "                                        use_crf=True)\n",
    "#tagger = SequenceTagger.load('/path/to/model.pt')                              # to load model from .pt file\n",
    "  \n",
    "# 5. initialize trainer\n",
    "\n",
    "trainer: ModelTrainer = ModelTrainer(tagger, corpus)\n",
    "  \n",
    "# 6. start training\n",
    "trainer.train('resources/taggers/example-ner'\n",
    "              ,learning_rate=0.1\n",
    "              ,mini_batch_size=64\n",
    "              ,max_epochs=5\n",
    "             #,checkpoint=True                                                  # to stop and resume training in case of large models!\n",
    "             )\n",
    "\n",
    "# 7. plot training curves (optional)                                            # need to locate files on disk to work!\n",
    "# plotter = Plotter()\n",
    "# plotter.plot_training_curves('loss.tsv')\n",
    "# plotter.plot_weights('weights.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S7wxr0eAkdOV"
   },
   "source": [
    "# NER on WIKINER_ENGLISH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 860500,
     "status": "ok",
     "timestamp": 1562926551943,
     "user": {
      "displayName": "George Gousios",
      "photoUrl": "",
      "userId": "00925390190798313342"
     },
     "user_tz": -120
    },
    "id": "5468gykEklQQ",
    "outputId": "b5f3d961-aeb6-4390-c1f2-f82f5484e70f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-07-12 09:51:24,770 Reading data from /root/.flair/datasets/wikiner_english\n",
      "2019-07-12 09:51:24,772 Train: /root/.flair/datasets/wikiner_english/aij-wikiner-en-wp3.train\n",
      "2019-07-12 09:51:24,773 Dev: None\n",
      "2019-07-12 09:51:24,774 Test: None\n",
      "Corpus: 11514 train + 1279 dev + 1422 test sentences\n",
      "[b'<unk>', b'O', b'B-ORG', b'E-ORG', b'S-ORG', b'S-LOC', b'S-PER', b'B-LOC', b'E-LOC', b'B-MISC', b'E-MISC', b'S-MISC', b'B-PER', b'E-PER', b'I-MISC', b'I-PER', b'I-ORG', b'I-LOC', b'<START>', b'<STOP>']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-07-12 09:52:07,751 https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings-v0.4.1/big-news-forward--h2048-l1-d0.05-lr30-0.25-20/news-forward-0.4.1.pt not found in cache, downloading to /tmp/tmpbnvua6c_\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 73034624/73034624 [00:04<00:00, 15843433.68B/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-07-12 09:52:12,820 copying /tmp/tmpbnvua6c_ to cache at /root/.flair/embeddings/news-forward-0.4.1.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-07-12 09:52:12,911 removing temp file /tmp/tmpbnvua6c_\n",
      "2019-07-12 09:52:13,927 https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings-v0.4.1/big-news-backward--h2048-l1-d0.05-lr30-0.25-20/news-backward-0.4.1.pt not found in cache, downloading to /tmp/tmp3cy9r8nw\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 73034575/73034575 [00:04<00:00, 16848970.40B/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-07-12 09:52:18,823 copying /tmp/tmp3cy9r8nw to cache at /root/.flair/embeddings/news-backward-0.4.1.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-07-12 09:52:18,904 removing temp file /tmp/tmp3cy9r8nw\n",
      "2019-07-12 09:52:19,484 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-12 09:52:19,486 Evaluation method: MICRO_F1_SCORE\n",
      "2019-07-12 09:52:20,134 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-12 09:52:22,857 epoch 1 - iter 0/180 - loss 83.74026489\n",
      "2019-07-12 09:52:41,735 epoch 1 - iter 18/180 - loss 23.84901428\n",
      "2019-07-12 09:53:01,097 epoch 1 - iter 36/180 - loss 18.43657202\n",
      "2019-07-12 09:53:22,616 epoch 1 - iter 54/180 - loss 15.74591389\n",
      "2019-07-12 09:53:41,659 epoch 1 - iter 72/180 - loss 13.75866139\n",
      "2019-07-12 09:54:02,691 epoch 1 - iter 90/180 - loss 12.37682143\n",
      "2019-07-12 09:54:22,523 epoch 1 - iter 108/180 - loss 11.36019380\n",
      "2019-07-12 09:54:42,604 epoch 1 - iter 126/180 - loss 10.50427462\n",
      "2019-07-12 09:55:01,799 epoch 1 - iter 144/180 - loss 9.82579675\n",
      "2019-07-12 09:55:21,525 epoch 1 - iter 162/180 - loss 9.28988408\n",
      "2019-07-12 09:55:39,923 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-12 09:55:39,925 EPOCH 1 done: loss 8.8434 - lr 0.1000 - bad epochs 0\n",
      "2019-07-12 09:56:07,595 DEV : loss 3.4822804927825928 - score 0.7198\n",
      "2019-07-12 09:56:39,256 TEST : loss 3.672832489013672 - score 0.718\n",
      "2019-07-12 09:56:43,926 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-12 09:56:56,946 epoch 2 - iter 0/180 - loss 3.35576725\n",
      "2019-07-12 09:57:16,366 epoch 2 - iter 18/180 - loss 3.98034904\n",
      "2019-07-12 09:57:36,151 epoch 2 - iter 36/180 - loss 3.98915373\n",
      "2019-07-12 09:57:56,613 epoch 2 - iter 54/180 - loss 3.96315911\n",
      "2019-07-12 09:58:17,451 epoch 2 - iter 72/180 - loss 3.95541479\n",
      "2019-07-12 09:58:37,506 epoch 2 - iter 90/180 - loss 3.90451913\n",
      "2019-07-12 09:58:58,186 epoch 2 - iter 108/180 - loss 3.85359157\n",
      "2019-07-12 09:59:18,369 epoch 2 - iter 126/180 - loss 3.79969010\n",
      "2019-07-12 09:59:38,255 epoch 2 - iter 144/180 - loss 3.75769435\n",
      "2019-07-12 09:59:57,503 epoch 2 - iter 162/180 - loss 3.71601621\n",
      "2019-07-12 10:00:18,278 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-12 10:00:18,280 EPOCH 2 done: loss 3.6754 - lr 0.1000 - bad epochs 0\n",
      "2019-07-12 10:00:45,757 DEV : loss 2.683284282684326 - score 0.7526\n",
      "2019-07-12 10:01:17,642 TEST : loss 2.8165321350097656 - score 0.7526\n",
      "2019-07-12 10:01:22,532 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-12 10:01:35,408 epoch 3 - iter 0/180 - loss 3.02495861\n",
      "2019-07-12 10:01:56,403 epoch 3 - iter 18/180 - loss 3.47212041\n",
      "2019-07-12 10:02:16,054 epoch 3 - iter 36/180 - loss 3.34655195\n",
      "2019-07-12 10:02:36,048 epoch 3 - iter 54/180 - loss 3.22794564\n",
      "2019-07-12 10:02:56,440 epoch 3 - iter 72/180 - loss 3.22737801\n",
      "2019-07-12 10:03:15,480 epoch 3 - iter 90/180 - loss 3.19012763\n",
      "2019-07-12 10:03:35,137 epoch 3 - iter 108/180 - loss 3.13980493\n",
      "2019-07-12 10:03:56,057 epoch 3 - iter 126/180 - loss 3.10253376\n",
      "2019-07-12 10:04:17,584 epoch 3 - iter 144/180 - loss 3.08369824\n",
      "2019-07-12 10:04:35,791 epoch 3 - iter 162/180 - loss 3.06118251\n",
      "2019-07-12 10:04:55,163 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-12 10:04:55,165 EPOCH 3 done: loss 3.0533 - lr 0.1000 - bad epochs 0\n",
      "2019-07-12 10:05:22,550 DEV : loss 2.237227201461792 - score 0.7686\n",
      "2019-07-12 10:05:55,026 TEST : loss 2.3985486030578613 - score 0.7835\n",
      "2019-07-12 10:05:59,765 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-12 10:06:02,362 epoch 4 - iter 0/180 - loss 2.76945543\n",
      "2019-07-12 10:06:21,943 epoch 4 - iter 18/180 - loss 2.88447130\n",
      "2019-07-12 10:06:42,005 epoch 4 - iter 36/180 - loss 2.83891318\n",
      "2019-07-12 10:07:01,690 epoch 4 - iter 54/180 - loss 2.79536695\n",
      "2019-07-12 10:07:22,514 epoch 4 - iter 72/180 - loss 2.78234876\n",
      "2019-07-12 10:07:43,471 epoch 4 - iter 90/180 - loss 2.76796925\n",
      "2019-07-12 10:08:04,781 epoch 4 - iter 108/180 - loss 2.73881035\n",
      "2019-07-12 10:08:24,891 epoch 4 - iter 126/180 - loss 2.71292538\n",
      "2019-07-12 10:08:45,139 epoch 4 - iter 144/180 - loss 2.71240193\n",
      "2019-07-12 10:09:04,964 epoch 4 - iter 162/180 - loss 2.70530118\n",
      "2019-07-12 10:09:23,326 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-12 10:09:23,327 EPOCH 4 done: loss 2.6780 - lr 0.1000 - bad epochs 0\n",
      "2019-07-12 10:09:50,914 DEV : loss 2.017707586288452 - score 0.7841\n",
      "2019-07-12 10:10:23,356 TEST : loss 2.1377005577087402 - score 0.7972\n",
      "2019-07-12 10:10:28,125 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-12 10:10:31,177 epoch 5 - iter 0/180 - loss 2.87436056\n",
      "2019-07-12 10:10:52,704 epoch 5 - iter 18/180 - loss 2.70717207\n",
      "2019-07-12 10:11:12,834 epoch 5 - iter 36/180 - loss 2.65103705\n",
      "2019-07-12 10:11:32,233 epoch 5 - iter 54/180 - loss 2.57500798\n",
      "2019-07-12 10:11:52,798 epoch 5 - iter 72/180 - loss 2.56403252\n",
      "2019-07-12 10:12:12,217 epoch 5 - iter 90/180 - loss 2.54607169\n",
      "2019-07-12 10:12:32,194 epoch 5 - iter 108/180 - loss 2.52858227\n",
      "2019-07-12 10:12:54,137 epoch 5 - iter 126/180 - loss 2.52790476\n",
      "2019-07-12 10:13:15,132 epoch 5 - iter 144/180 - loss 2.50510444\n",
      "2019-07-12 10:13:33,470 epoch 5 - iter 162/180 - loss 2.49756734\n",
      "2019-07-12 10:13:51,313 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-12 10:13:51,314 EPOCH 5 done: loss 2.4770 - lr 0.1000 - bad epochs 0\n",
      "2019-07-12 10:14:18,761 DEV : loss 1.9837785959243774 - score 0.7832\n",
      "2019-07-12 10:14:50,105 TEST : loss 2.1400864124298096 - score 0.793\n",
      "2019-07-12 10:14:54,863 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-12 10:14:54,874 Testing using best model ...\n",
      "2019-07-12 10:14:54,876 loading file resources/taggers/example-ner/best-model.pt\n",
      "2019-07-12 10:15:29,509 0.797\t0.7975\t0.7972\n",
      "2019-07-12 10:15:29,512 \n",
      "MICRO_AVG: acc 0.6628 - f1-score 0.7972\n",
      "MACRO_AVG: acc 0.6485 - f1-score 0.77945\n",
      "LOC        tp: 738 - fp: 170 - fn: 132 - tn: 738 - precision: 0.8128 - recall: 0.8483 - accuracy: 0.7096 - f1-score: 0.8302\n",
      "MISC       tp: 488 - fp: 260 - fn: 236 - tn: 488 - precision: 0.6524 - recall: 0.6740 - accuracy: 0.4959 - f1-score: 0.6630\n",
      "ORG        tp: 340 - fp: 84 - fn: 181 - tn: 340 - precision: 0.8019 - recall: 0.6526 - accuracy: 0.5620 - f1-score: 0.7196\n",
      "PER        tp: 891 - fp: 112 - fn: 75 - tn: 891 - precision: 0.8883 - recall: 0.9224 - accuracy: 0.8265 - f1-score: 0.9050\n",
      "2019-07-12 10:15:29,518 ----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 1. get the corpus\n",
    "corpus: Corpus = WIKINER_ENGLISH().downsample(0.1)\n",
    "print(corpus)\n",
    "\n",
    "# 2. what tag do we want to predict?\n",
    "tag_type = 'ner'\n",
    "\n",
    "# 3. make the tag dictionary from the corpus\n",
    "tag_dictionary = corpus.make_tag_dictionary(tag_type=tag_type)\n",
    "print(tag_dictionary.idx2item)\n",
    "\n",
    "# 4. initialize embeddings\n",
    "embedding_types: List[TokenEmbeddings] = [\n",
    "\n",
    "    WordEmbeddings('glove'),\n",
    "    # comment in this line to use character embeddings\n",
    "    # CharacterEmbeddings(),\n",
    "\n",
    "    # comment in these lines to use flair embeddings\n",
    "    FlairEmbeddings('news-forward'),                                            # Stacked Glove + Flair embeddings supposedly gives the best performance\n",
    "    FlairEmbeddings('news-backward'),\n",
    "]\n",
    "\n",
    "embeddings: StackedEmbeddings = StackedEmbeddings(embeddings=embedding_types)\n",
    "\n",
    "# 5. initialize sequence tagger\n",
    "from flair.models import SequenceTagger\n",
    "\n",
    "tagger: SequenceTagger = SequenceTagger(hidden_size=256,\n",
    "                                        embeddings=embeddings,\n",
    "                                        tag_dictionary=tag_dictionary,\n",
    "                                        tag_type=tag_type,\n",
    "                                        use_crf=True)\n",
    "\n",
    "# 6. initialize trainer\n",
    "from flair.trainers import ModelTrainer\n",
    "\n",
    "trainer: ModelTrainer = ModelTrainer(tagger, corpus)\n",
    "\n",
    "# 7. start training\n",
    "trainer.train('resources/taggers/example-ner',\n",
    "              learning_rate=0.1,\n",
    "              mini_batch_size=64,\n",
    "              max_epochs=5)\n",
    "\n",
    "# 8. plot training curves (optional)\n",
    "from flair.visual.training_curves import Plotter\n",
    "plotter = Plotter()\n",
    "plotter.plot_training_curves('resources/taggers/example-ner/loss.tsv')\n",
    "plotter.plot_weights('resources/taggers/example-ner/weights.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T1xVOu_3lG3a"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "3PWGk8iJBsMX",
    "43dv3thz45sq",
    "S7wxr0eAkdOV"
   ],
   "name": "Flair Sequence Tagging + Text Classification.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
