{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flair RNN \n",
    "\n",
    "This notebook contains the attempt of the text classification using the Flair NLP Framework [link|https://github.com/flairNLP/flair] which internally uses the pytorch ml library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.cuda.device at 0x7f932436d290>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.device(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 2;\n",
       "                var nbb_unformatted_code = \"import os\\nimport random\\nimport pandas as pd\\nimport numpy as np\\nimport parent_modules\\nimport preprocessor\\nfrom collections import OrderedDict\\nfrom sklearn.model_selection import train_test_split\\nfrom collections import Counter\\nfrom sklearn.datasets import make_classification\\nfrom imblearn.combine import SMOTEENN\\n\\n\\n%load_ext autoreload\\n%load_ext nb_black\\n%autoreload 2\\n\\nfrom definitions import *\\n\\nTESTING = True\";\n",
       "                var nbb_formatted_code = \"import os\\nimport random\\nimport pandas as pd\\nimport numpy as np\\nimport parent_modules\\nimport preprocessor\\nfrom collections import OrderedDict\\nfrom sklearn.model_selection import train_test_split\\nfrom collections import Counter\\nfrom sklearn.datasets import make_classification\\nfrom imblearn.combine import SMOTEENN\\n\\n\\n%load_ext autoreload\\n%load_ext nb_black\\n%autoreload 2\\n\\nfrom definitions import *\\n\\nTESTING = True\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import parent_modules\n",
    "import preprocessor\n",
    "from collections import OrderedDict\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "from sklearn.datasets import make_classification\n",
    "from imblearn.combine import SMOTEENN\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%load_ext nb_black\n",
    "%autoreload 2\n",
    "\n",
    "from definitions import *\n",
    "\n",
    "TESTING = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "> Data loading and transformation for the correct usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/giannhs/miniconda3/envs/data_challenge/lib/python3.7/site-packages/ipykernel_launcher.py:3: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 3;\n",
       "                var nbb_unformatted_code = \"datasets = {\\n    \\\"posts\\\": pd.read_csv(\\n        os.path.join(DATA_DIR, \\\"posts.tsv\\\"), sep=\\\"\\\\t|\\\\t \\\", header=None\\n    ),\\n    \\\"test\\\": pd.read_csv(os.path.join(DATA_DIR, \\\"test.csv\\\"), header=None),\\n    \\\"train\\\": pd.read_csv(os.path.join(DATA_DIR, \\\"train.csv\\\"), header=None),\\n    \\\"users\\\": pd.read_csv(os.path.join(DATA_DIR, \\\"users.csv\\\")),\\n}\\ndatasets[\\\"posts\\\"].columns = [\\\"post_id\\\", \\\"user_id\\\", \\\"post\\\"]\\ndatasets[\\\"test\\\"].columns = [\\\"post_id\\\", \\\"label\\\"]\\ndatasets[\\\"train\\\"].columns = [\\\"post_id\\\", \\\"label\\\"]\\n\\ntrain_ids = datasets[\\\"train\\\"][\\\"post_id\\\"]\\ntest_ids = datasets[\\\"test\\\"][\\\"post_id\\\"]\\ntrain_posts = datasets[\\\"posts\\\"][datasets[\\\"posts\\\"].post_id.isin(list(train_ids))].post\\ndatasets[\\\"train\\\"].insert(2, \\\"post\\\", list(train_posts))\\ntests_posts = datasets[\\\"posts\\\"][datasets[\\\"posts\\\"].post_id.isin(list(test_ids))].post\\ndatasets[\\\"test\\\"].insert(2, \\\"post\\\", list(tests_posts))\\n\\nflair_full_train = datasets[\\\"train\\\"].copy(deep=True)\\nflair_full_train[\\\"label\\\"] = \\\"__label__\\\" + datasets[\\\"train\\\"][\\\"label\\\"].astype(str)\";\n",
       "                var nbb_formatted_code = \"datasets = {\\n    \\\"posts\\\": pd.read_csv(\\n        os.path.join(DATA_DIR, \\\"posts.tsv\\\"), sep=\\\"\\\\t|\\\\t \\\", header=None\\n    ),\\n    \\\"test\\\": pd.read_csv(os.path.join(DATA_DIR, \\\"test.csv\\\"), header=None),\\n    \\\"train\\\": pd.read_csv(os.path.join(DATA_DIR, \\\"train.csv\\\"), header=None),\\n    \\\"users\\\": pd.read_csv(os.path.join(DATA_DIR, \\\"users.csv\\\")),\\n}\\ndatasets[\\\"posts\\\"].columns = [\\\"post_id\\\", \\\"user_id\\\", \\\"post\\\"]\\ndatasets[\\\"test\\\"].columns = [\\\"post_id\\\", \\\"label\\\"]\\ndatasets[\\\"train\\\"].columns = [\\\"post_id\\\", \\\"label\\\"]\\n\\ntrain_ids = datasets[\\\"train\\\"][\\\"post_id\\\"]\\ntest_ids = datasets[\\\"test\\\"][\\\"post_id\\\"]\\ntrain_posts = datasets[\\\"posts\\\"][datasets[\\\"posts\\\"].post_id.isin(list(train_ids))].post\\ndatasets[\\\"train\\\"].insert(2, \\\"post\\\", list(train_posts))\\ntests_posts = datasets[\\\"posts\\\"][datasets[\\\"posts\\\"].post_id.isin(list(test_ids))].post\\ndatasets[\\\"test\\\"].insert(2, \\\"post\\\", list(tests_posts))\\n\\nflair_full_train = datasets[\\\"train\\\"].copy(deep=True)\\nflair_full_train[\\\"label\\\"] = \\\"__label__\\\" + datasets[\\\"train\\\"][\\\"label\\\"].astype(str)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "datasets = {\n",
    "    \"posts\": pd.read_csv(\n",
    "        os.path.join(DATA_DIR, \"posts.tsv\"), sep=\"\\t|\\t \", header=None\n",
    "    ),\n",
    "    \"test\": pd.read_csv(os.path.join(DATA_DIR, \"test.csv\"), header=None),\n",
    "    \"train\": pd.read_csv(os.path.join(DATA_DIR, \"train.csv\"), header=None),\n",
    "    \"users\": pd.read_csv(os.path.join(DATA_DIR, \"users.csv\")),\n",
    "}\n",
    "datasets[\"posts\"].columns = [\"post_id\", \"user_id\", \"post\"]\n",
    "datasets[\"test\"].columns = [\"post_id\", \"label\"]\n",
    "datasets[\"train\"].columns = [\"post_id\", \"label\"]\n",
    "\n",
    "train_ids = datasets[\"train\"][\"post_id\"]\n",
    "test_ids = datasets[\"test\"][\"post_id\"]\n",
    "train_posts = datasets[\"posts\"][datasets[\"posts\"].post_id.isin(list(train_ids))].post\n",
    "datasets[\"train\"].insert(2, \"post\", list(train_posts))\n",
    "tests_posts = datasets[\"posts\"][datasets[\"posts\"].post_id.isin(list(test_ids))].post\n",
    "datasets[\"test\"].insert(2, \"post\", list(tests_posts))\n",
    "\n",
    "flair_full_train = datasets[\"train\"].copy(deep=True)\n",
    "flair_full_train[\"label\"] = \"__label__\" + datasets[\"train\"][\"label\"].astype(str)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "> Data splitting and Preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 4;\n",
       "                var nbb_unformatted_code = \"# clean posts & find weights\\nflair_full_train[\\\"post\\\"] = flair_full_train[\\\"post\\\"].apply(\\n    lambda text: preprocessor.clean(text)\\n)\\ndatasets[\\\"test\\\"][\\\"post\\\"] = flair_full_train[\\\"post\\\"].apply(\\n    lambda text: preprocessor.clean(text)\\n)\\n\\ndatasets[\\\"test\\\"][\\\"post\\\"] = datasets[\\\"test\\\"].post.apply(lambda post: post.strip())\\ndatasets[\\\"test\\\"][\\\"post\\\"] = datasets[\\\"test\\\"][\\\"post\\\"].replace([None, \\\"\\\"], \\\"NaN\\\")\\n\\nclasses_counts = (\\n    flair_full_train[\\\"label\\\"].value_counts().where(lambda cls: cls > 0).dropna()\\n)\\n\\n# Apply Find the weights\\ntotal_entries = sum(classes_counts)\\nclass_unordered_weight = {}\\nfor label, cls_entries in zip(classes_counts.index, classes_counts):\\n    class_num = bytes(label.split(\\\"__label__\\\")[1], \\\"utf-8\\\")\\n    class_unordered_weight[class_num] = np.round(\\n        (1 / cls_entries) * total_entries / 2, 4\\n    )\\n\\nclass_weight = OrderedDict()\\nfor class_num in range(15):\\n    class_num = bytes(str(class_num), \\\"utf-8\\\")\\n    class_weight[class_num] = class_unordered_weight[class_num]\\n\\n# split trainset to dev and train\\nif TESTING:\\n    big_train, small_train = train_test_split(\\n        flair_full_train,\\n        test_size=0.5,\\n        random_state=np.random.RandomState(12),\\n        stratify=flair_full_train[\\\"label\\\"],\\n    )\\n    flair_train, flair_test = train_test_split(\\n        small_train,\\n        test_size=0.2,\\n        random_state=np.random.RandomState(12),\\n        stratify=small_train[\\\"label\\\"],\\n    )\\n\\nelse:\\n    flair_train, flair_test = train_test_split(\\n        flair_full_train,\\n        test_size=0.2,\\n        random_state=np.random.RandomState(12),\\n        stratify=flair_full_train[\\\"label\\\"],\\n    )\";\n",
       "                var nbb_formatted_code = \"# clean posts & find weights\\nflair_full_train[\\\"post\\\"] = flair_full_train[\\\"post\\\"].apply(\\n    lambda text: preprocessor.clean(text)\\n)\\ndatasets[\\\"test\\\"][\\\"post\\\"] = flair_full_train[\\\"post\\\"].apply(\\n    lambda text: preprocessor.clean(text)\\n)\\n\\ndatasets[\\\"test\\\"][\\\"post\\\"] = datasets[\\\"test\\\"].post.apply(lambda post: post.strip())\\ndatasets[\\\"test\\\"][\\\"post\\\"] = datasets[\\\"test\\\"][\\\"post\\\"].replace([None, \\\"\\\"], \\\"NaN\\\")\\n\\nclasses_counts = (\\n    flair_full_train[\\\"label\\\"].value_counts().where(lambda cls: cls > 0).dropna()\\n)\\n\\n# Apply Find the weights\\ntotal_entries = sum(classes_counts)\\nclass_unordered_weight = {}\\nfor label, cls_entries in zip(classes_counts.index, classes_counts):\\n    class_num = bytes(label.split(\\\"__label__\\\")[1], \\\"utf-8\\\")\\n    class_unordered_weight[class_num] = np.round(\\n        (1 / cls_entries) * total_entries / 2, 4\\n    )\\n\\nclass_weight = OrderedDict()\\nfor class_num in range(15):\\n    class_num = bytes(str(class_num), \\\"utf-8\\\")\\n    class_weight[class_num] = class_unordered_weight[class_num]\\n\\n# split trainset to dev and train\\nif TESTING:\\n    big_train, small_train = train_test_split(\\n        flair_full_train,\\n        test_size=0.5,\\n        random_state=np.random.RandomState(12),\\n        stratify=flair_full_train[\\\"label\\\"],\\n    )\\n    flair_train, flair_test = train_test_split(\\n        small_train,\\n        test_size=0.2,\\n        random_state=np.random.RandomState(12),\\n        stratify=small_train[\\\"label\\\"],\\n    )\\n\\nelse:\\n    flair_train, flair_test = train_test_split(\\n        flair_full_train,\\n        test_size=0.2,\\n        random_state=np.random.RandomState(12),\\n        stratify=flair_full_train[\\\"label\\\"],\\n    )\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# clean posts & find weights\n",
    "flair_full_train[\"post\"] = flair_full_train[\"post\"].apply(\n",
    "    lambda text: preprocessor.clean(text)\n",
    ")\n",
    "datasets[\"test\"][\"post\"] = flair_full_train[\"post\"].apply(\n",
    "    lambda text: preprocessor.clean(text)\n",
    ")\n",
    "\n",
    "datasets[\"test\"][\"post\"] = datasets[\"test\"].post.apply(lambda post: post.strip())\n",
    "datasets[\"test\"][\"post\"] = datasets[\"test\"][\"post\"].replace([None, \"\"], \"NaN\")\n",
    "\n",
    "classes_counts = (\n",
    "    flair_full_train[\"label\"].value_counts().where(lambda cls: cls > 0).dropna()\n",
    ")\n",
    "\n",
    "# Apply Find the weights\n",
    "total_entries = sum(classes_counts)\n",
    "class_unordered_weight = {}\n",
    "for label, cls_entries in zip(classes_counts.index, classes_counts):\n",
    "    class_num = bytes(label.split(\"__label__\")[1], \"utf-8\")\n",
    "    class_unordered_weight[class_num] = np.round(\n",
    "        (1 / cls_entries) * total_entries / 2, 4\n",
    "    )\n",
    "\n",
    "class_weight = OrderedDict()\n",
    "for class_num in range(15):\n",
    "    class_num = bytes(str(class_num), \"utf-8\")\n",
    "    class_weight[class_num] = class_unordered_weight[class_num]\n",
    "\n",
    "# split trainset to dev and train\n",
    "if TESTING:\n",
    "    big_train, small_train = train_test_split(\n",
    "        flair_full_train,\n",
    "        test_size=0.5,\n",
    "        random_state=np.random.RandomState(12),\n",
    "        stratify=flair_full_train[\"label\"],\n",
    "    )\n",
    "    flair_train, flair_test = train_test_split(\n",
    "        small_train,\n",
    "        test_size=0.2,\n",
    "        random_state=np.random.RandomState(12),\n",
    "        stratify=small_train[\"label\"],\n",
    "    )\n",
    "\n",
    "else:\n",
    "    flair_train, flair_test = train_test_split(\n",
    "        flair_full_train,\n",
    "        test_size=0.2,\n",
    "        random_state=np.random.RandomState(12),\n",
    "        stratify=flair_full_train[\"label\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "> Save the new csv files which will be loaded to the corpus of the flair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 5;\n",
       "                var nbb_unformatted_code = \"flair_dev, flair_test = train_test_split(\\n    flair_test,\\n    test_size=0.5,\\n    random_state=np.random.RandomState(12),\\n    stratify=flair_test[\\\"label\\\"],\\n)\\n\\n\\n# save as_csv\\nflair_train.to_csv(\\n    os.path.join(FLAIR_DATA_DIR, \\\"flair_train.csv\\\"),\\n    sep=\\\"\\\\t\\\",\\n    index=False,\\n    header=False,\\n    columns=[\\\"label\\\", \\\"post\\\"],\\n)\\nflair_dev.to_csv(\\n    os.path.join(FLAIR_DATA_DIR, \\\"flair_dev.csv\\\"),\\n    sep=\\\"\\\\t\\\",\\n    index=False,\\n    header=False,\\n    columns=[\\\"label\\\", \\\"post\\\"],\\n)\\nflair_test.to_csv(\\n    os.path.join(FLAIR_DATA_DIR, \\\"flair_test.csv\\\"),\\n    sep=\\\"\\\\t\\\",\\n    index=False,\\n    header=False,\\n    columns=[\\\"label\\\", \\\"post\\\"],\\n)\";\n",
       "                var nbb_formatted_code = \"flair_dev, flair_test = train_test_split(\\n    flair_test,\\n    test_size=0.5,\\n    random_state=np.random.RandomState(12),\\n    stratify=flair_test[\\\"label\\\"],\\n)\\n\\n\\n# save as_csv\\nflair_train.to_csv(\\n    os.path.join(FLAIR_DATA_DIR, \\\"flair_train.csv\\\"),\\n    sep=\\\"\\\\t\\\",\\n    index=False,\\n    header=False,\\n    columns=[\\\"label\\\", \\\"post\\\"],\\n)\\nflair_dev.to_csv(\\n    os.path.join(FLAIR_DATA_DIR, \\\"flair_dev.csv\\\"),\\n    sep=\\\"\\\\t\\\",\\n    index=False,\\n    header=False,\\n    columns=[\\\"label\\\", \\\"post\\\"],\\n)\\nflair_test.to_csv(\\n    os.path.join(FLAIR_DATA_DIR, \\\"flair_test.csv\\\"),\\n    sep=\\\"\\\\t\\\",\\n    index=False,\\n    header=False,\\n    columns=[\\\"label\\\", \\\"post\\\"],\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "flair_dev, flair_test = train_test_split(\n",
    "    flair_test,\n",
    "    test_size=0.5,\n",
    "    random_state=np.random.RandomState(12),\n",
    "    stratify=flair_test[\"label\"],\n",
    ")\n",
    "\n",
    "\n",
    "# save as_csv\n",
    "flair_train.to_csv(\n",
    "    os.path.join(FLAIR_DATA_DIR, \"flair_train.csv\"),\n",
    "    sep=\"\\t\",\n",
    "    index=False,\n",
    "    header=False,\n",
    "    columns=[\"label\", \"post\"],\n",
    ")\n",
    "flair_dev.to_csv(\n",
    "    os.path.join(FLAIR_DATA_DIR, \"flair_dev.csv\"),\n",
    "    sep=\"\\t\",\n",
    "    index=False,\n",
    "    header=False,\n",
    "    columns=[\"label\", \"post\"],\n",
    ")\n",
    "flair_test.to_csv(\n",
    "    os.path.join(FLAIR_DATA_DIR, \"flair_test.csv\"),\n",
    "    sep=\"\\t\",\n",
    "    index=False,\n",
    "    header=False,\n",
    "    columns=[\"label\", \"post\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([(b'0', 1.1758),\n",
       "             (b'1', 11.5366),\n",
       "             (b'2', 2.781),\n",
       "             (b'3', 4.7218),\n",
       "             (b'4', 6.207),\n",
       "             (b'5', 36.725),\n",
       "             (b'6', 70.3245),\n",
       "             (b'7', 367.25),\n",
       "             (b'8', 54.1844),\n",
       "             (b'9', 58.5),\n",
       "             (b'10', 13.1946),\n",
       "             (b'11', 20.4028),\n",
       "             (b'12', 22.1087),\n",
       "             (b'13', 31.9348),\n",
       "             (b'14', 20.2776)])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 5;\n",
       "                var nbb_unformatted_code = \"class_weight\";\n",
       "                var nbb_formatted_code = \"class_weight\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Flair Load Embeddings\n",
    "\n",
    "### Instractions\n",
    "> In case you haven't download the used embeddings then click on the below links and place them in the *data/flair_files/* folder\n",
    "\n",
    "#### Twitter Embeddings\n",
    "1. https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings/twitter.gensim.vectors.npy\n",
    "2. https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings/twitter.gensim\n",
    "\n",
    "#### News Forward English\n",
    "1. https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings/lm-news-english-forward-1024-v0.2rc.pt\n",
    "\n",
    "#### News Backward English\n",
    "1. https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings/lm-news-english-backward-1024-v0.2rc.pt\n",
    "\n",
    "#### Glove\n",
    "1. https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings/glove.gensim.vectors.npy\n",
    "2. https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings/glove.gensim\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/giannhs/miniconda3/envs/data_challenge/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/giannhs/miniconda3/envs/data_challenge/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/giannhs/miniconda3/envs/data_challenge/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/giannhs/miniconda3/envs/data_challenge/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/giannhs/miniconda3/envs/data_challenge/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/giannhs/miniconda3/envs/data_challenge/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/giannhs/miniconda3/envs/data_challenge/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/giannhs/miniconda3/envs/data_challenge/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/giannhs/miniconda3/envs/data_challenge/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/giannhs/miniconda3/envs/data_challenge/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/giannhs/miniconda3/envs/data_challenge/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/giannhs/miniconda3/envs/data_challenge/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 6;\n",
       "                var nbb_unformatted_code = \"from flair.embeddings import (\\n    StackedEmbeddings,\\n    DocumentRNNEmbeddings,\\n    WordEmbeddings,\\n    FlairEmbeddings,\\n)\\nfrom flair.datasets import ClassificationCorpus\\nfrom flair.models import TextClassifier\\nfrom flair.trainers import ModelTrainer\\nfrom flair.data import Sentence\\nfrom flair.visual.training_curves import Plotter\\nfrom torch.utils.data.sampler import WeightedRandomSampler\\n\\n\\noutput_folder = os.path.join(FLAIR_OUTPUT_DIR, \\\"rnn_flair_basic\\\")\\nnew_model_folder = os.path.join(FLAIR_OUTPUT_DIR, \\\"flair_training_model\\\")\";\n",
       "                var nbb_formatted_code = \"from flair.embeddings import (\\n    StackedEmbeddings,\\n    DocumentRNNEmbeddings,\\n    WordEmbeddings,\\n    FlairEmbeddings,\\n)\\nfrom flair.datasets import ClassificationCorpus\\nfrom flair.models import TextClassifier\\nfrom flair.trainers import ModelTrainer\\nfrom flair.data import Sentence\\nfrom flair.visual.training_curves import Plotter\\nfrom torch.utils.data.sampler import WeightedRandomSampler\\n\\n\\noutput_folder = os.path.join(FLAIR_OUTPUT_DIR, \\\"rnn_flair_basic\\\")\\nnew_model_folder = os.path.join(FLAIR_OUTPUT_DIR, \\\"flair_training_model\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from flair.embeddings import (\n",
    "    StackedEmbeddings,\n",
    "    DocumentRNNEmbeddings,\n",
    "    WordEmbeddings,\n",
    "    FlairEmbeddings,\n",
    ")\n",
    "from flair.datasets import ClassificationCorpus\n",
    "from flair.models import TextClassifier\n",
    "from flair.trainers import ModelTrainer\n",
    "from flair.data import Sentence\n",
    "from flair.visual.training_curves import Plotter\n",
    "from torch.utils.data.sampler import WeightedRandomSampler\n",
    "\n",
    "\n",
    "output_folder = os.path.join(FLAIR_OUTPUT_DIR, \"rnn_flair_basic\")\n",
    "new_model_folder = os.path.join(FLAIR_OUTPUT_DIR, \"flair_training_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Custom Loss Function \n",
    "\n",
    "#### Focal Loss - Dense Loss function for imbalanced Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 7;\n",
       "                var nbb_unformatted_code = \"import torch\\nimport torch.nn as nn\\nimport torch.nn.functional as F\\nfrom torch.autograd import Variable\\n\\n\\nclass FocalLoss(nn.Module):\\n    def __init__(self, gamma=0, alpha=None, size_average=True):\\n        super(FocalLoss, self).__init__()\\n        self.gamma = gamma\\n        self.alpha = alpha\\n        if isinstance(alpha, (float, int)):\\n            self.alpha = torch.Tensor([alpha, 1 - alpha])\\n        if isinstance(alpha, list):\\n            self.alpha = torch.Tensor(alpha)\\n        self.size_average = size_average\\n\\n    def forward(self, input, target):\\n        if input.dim() > 2:\\n            input = input.view(input.size(0), input.size(1), -1)  # N,C,H,W => N,C,H*W\\n            input = input.transpose(1, 2)  # N,C,H*W => N,H*W,C\\n            input = input.contiguous().view(-1, input.size(2))  # N,H*W,C => N*H*W,C\\n        target = target.view(-1, 1)\\n\\n        logpt = F.log_softmax(input)\\n        logpt = logpt.gather(1, target)\\n        logpt = logpt.view(-1)\\n        pt = Variable(logpt.data.exp())\\n\\n        if self.alpha is not None:\\n            if self.alpha.type() != input.data.type():\\n                self.alpha = self.alpha.type_as(input.data)\\n            at = self.alpha.gather(0, target.data.view(-1))\\n            logpt = logpt * Variable(at)\\n\\n        loss = -1 * (1 - pt) ** self.gamma * logpt\\n        if self.size_average:\\n            return loss.mean()\\n        else:\\n            return loss.sum()\";\n",
       "                var nbb_formatted_code = \"import torch\\nimport torch.nn as nn\\nimport torch.nn.functional as F\\nfrom torch.autograd import Variable\\n\\n\\nclass FocalLoss(nn.Module):\\n    def __init__(self, gamma=0, alpha=None, size_average=True):\\n        super(FocalLoss, self).__init__()\\n        self.gamma = gamma\\n        self.alpha = alpha\\n        if isinstance(alpha, (float, int)):\\n            self.alpha = torch.Tensor([alpha, 1 - alpha])\\n        if isinstance(alpha, list):\\n            self.alpha = torch.Tensor(alpha)\\n        self.size_average = size_average\\n\\n    def forward(self, input, target):\\n        if input.dim() > 2:\\n            input = input.view(input.size(0), input.size(1), -1)  # N,C,H,W => N,C,H*W\\n            input = input.transpose(1, 2)  # N,C,H*W => N,H*W,C\\n            input = input.contiguous().view(-1, input.size(2))  # N,H*W,C => N*H*W,C\\n        target = target.view(-1, 1)\\n\\n        logpt = F.log_softmax(input)\\n        logpt = logpt.gather(1, target)\\n        logpt = logpt.view(-1)\\n        pt = Variable(logpt.data.exp())\\n\\n        if self.alpha is not None:\\n            if self.alpha.type() != input.data.type():\\n                self.alpha = self.alpha.type_as(input.data)\\n            at = self.alpha.gather(0, target.data.view(-1))\\n            logpt = logpt * Variable(at)\\n\\n        loss = -1 * (1 - pt) ** self.gamma * logpt\\n        if self.size_average:\\n            return loss.mean()\\n        else:\\n            return loss.sum()\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=0, alpha=None, size_average=True):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        if isinstance(alpha, (float, int)):\n",
    "            self.alpha = torch.Tensor([alpha, 1 - alpha])\n",
    "        if isinstance(alpha, list):\n",
    "            self.alpha = torch.Tensor(alpha)\n",
    "        self.size_average = size_average\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        if input.dim() > 2:\n",
    "            input = input.view(input.size(0), input.size(1), -1)  # N,C,H,W => N,C,H*W\n",
    "            input = input.transpose(1, 2)  # N,C,H*W => N,H*W,C\n",
    "            input = input.contiguous().view(-1, input.size(2))  # N,H*W,C => N*H*W,C\n",
    "        target = target.view(-1, 1)\n",
    "\n",
    "        logpt = F.log_softmax(input)\n",
    "        logpt = logpt.gather(1, target)\n",
    "        logpt = logpt.view(-1)\n",
    "        pt = Variable(logpt.data.exp())\n",
    "\n",
    "        if self.alpha is not None:\n",
    "            if self.alpha.type() != input.data.type():\n",
    "                self.alpha = self.alpha.type_as(input.data)\n",
    "            at = self.alpha.gather(0, target.data.view(-1))\n",
    "            logpt = logpt * Variable(at)\n",
    "\n",
    "        loss = -1 * (1 - pt) ** self.gamma * logpt\n",
    "        if self.size_average:\n",
    "            return loss.mean()\n",
    "        else:\n",
    "            return loss.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Word Embeddings using Word2Vec and Fasttext embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-23 20:05:16,628 Reading data from /home/giannhs/PycharmProjects/data_challenge/data/flair_data_dir\n",
      "2020-06-23 20:05:16,629 Train: /home/giannhs/PycharmProjects/data_challenge/data/flair_data_dir/flair_train.csv\n",
      "2020-06-23 20:05:16,630 Dev: /home/giannhs/PycharmProjects/data_challenge/data/flair_data_dir/flair_dev.csv\n",
      "2020-06-23 20:05:16,630 Test: /home/giannhs/PycharmProjects/data_challenge/data/flair_data_dir/flair_test.csv\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 7;\n",
       "                var nbb_unformatted_code = \"word_embeddings = [\\n    WordEmbeddings(os.path.join(FLAIR_EMDG_DIR, \\\"twitter.gensim\\\")),\\n    WordEmbeddings(os.path.join(FLAIR_EMDG_DIR, \\\"glove.gensim\\\")),\\n    FlairEmbeddings(\\n        os.path.join(FLAIR_EMDG_DIR, \\\"lm-news-english-forward-1024-v0.2rc.pt\\\")\\n    ),\\n    FlairEmbeddings(\\n        os.path.join(FLAIR_EMDG_DIR, \\\"lm-news-english-backward-1024-v0.2rc.pt\\\")\\n    ),\\n]\\n\\ncorpus = ClassificationCorpus(\\n    FLAIR_DATA_DIR,\\n    test_file=\\\"flair_test.csv\\\",\\n    dev_file=\\\"flair_dev.csv\\\",\\n    train_file=\\\"flair_train.csv\\\",\\n)\";\n",
       "                var nbb_formatted_code = \"word_embeddings = [\\n    WordEmbeddings(os.path.join(FLAIR_EMDG_DIR, \\\"twitter.gensim\\\")),\\n    WordEmbeddings(os.path.join(FLAIR_EMDG_DIR, \\\"glove.gensim\\\")),\\n    FlairEmbeddings(\\n        os.path.join(FLAIR_EMDG_DIR, \\\"lm-news-english-forward-1024-v0.2rc.pt\\\")\\n    ),\\n    FlairEmbeddings(\\n        os.path.join(FLAIR_EMDG_DIR, \\\"lm-news-english-backward-1024-v0.2rc.pt\\\")\\n    ),\\n]\\n\\ncorpus = ClassificationCorpus(\\n    FLAIR_DATA_DIR,\\n    test_file=\\\"flair_test.csv\\\",\\n    dev_file=\\\"flair_dev.csv\\\",\\n    train_file=\\\"flair_train.csv\\\",\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "word_embeddings = [\n",
    "    WordEmbeddings(os.path.join(FLAIR_EMDG_DIR, \"twitter.gensim\")),\n",
    "    WordEmbeddings(os.path.join(FLAIR_EMDG_DIR, \"glove.gensim\")),\n",
    "    FlairEmbeddings(\n",
    "        os.path.join(FLAIR_EMDG_DIR, \"lm-news-english-forward-1024-v0.2rc.pt\")\n",
    "    ),\n",
    "    FlairEmbeddings(\n",
    "        os.path.join(FLAIR_EMDG_DIR, \"lm-news-english-backward-1024-v0.2rc.pt\")\n",
    "    ),\n",
    "]\n",
    "\n",
    "corpus = ClassificationCorpus(\n",
    "    FLAIR_DATA_DIR,\n",
    "    test_file=\"flair_test.csv\",\n",
    "    dev_file=\"flair_dev.csv\",\n",
    "    train_file=\"flair_train.csv\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### RNN Model Design\n",
    "> Define the RNN Model parameters to along with the already constructed layers of the Flair Embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-23 20:05:33,852 Computing label dictionary. Progress:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5932/5932 [00:03<00:00, 1876.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-23 20:05:37,208 [b'12', b'0', b'4', b'2', b'3', b'10', b'11', b'5', b'13', b'14', b'1', b'9', b'8', b'6', b'7']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 8;\n",
       "                var nbb_unformatted_code = \"document_embeddings = DocumentRNNEmbeddings(\\n    word_embeddings,\\n    hidden_size=512,\\n    rnn_layers=2,\\n    dropout=0.2,\\n    reproject_words=True,\\n    rnn_type=\\\"GRU\\\",\\n    bidirectional=True,\\n    reproject_words_dimension=256,\\n)\\nclassifier = TextClassifier(\\n    document_embeddings,\\n    label_dictionary=corpus.make_label_dictionary(),\\n    loss_weights=class_weight,\\n    multi_label=False,\\n)\\n# Uncomment the below line if you want to run the network using the FocalLoss as Loss function.\\n# classifier.loss_fuction = FocalLoss(alpha=0.4, gamma=0.5)\";\n",
       "                var nbb_formatted_code = \"document_embeddings = DocumentRNNEmbeddings(\\n    word_embeddings,\\n    hidden_size=512,\\n    rnn_layers=2,\\n    dropout=0.2,\\n    reproject_words=True,\\n    rnn_type=\\\"GRU\\\",\\n    bidirectional=True,\\n    reproject_words_dimension=256,\\n)\\nclassifier = TextClassifier(\\n    document_embeddings,\\n    label_dictionary=corpus.make_label_dictionary(),\\n    loss_weights=class_weight,\\n    multi_label=False,\\n)\\n# Uncomment the below line if you want to run the network using the FocalLoss as Loss function.\\n# classifier.loss_fuction = FocalLoss(alpha=0.4, gamma=0.5)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "document_embeddings = DocumentRNNEmbeddings(\n",
    "    word_embeddings,\n",
    "    hidden_size=512,\n",
    "    rnn_layers=2,\n",
    "    dropout=0.2,\n",
    "    reproject_words=True,\n",
    "    rnn_type=\"GRU\",\n",
    "    bidirectional=True,\n",
    "    reproject_words_dimension=256,\n",
    ")\n",
    "classifier = TextClassifier(\n",
    "    document_embeddings,\n",
    "    label_dictionary=corpus.make_label_dictionary(),\n",
    "    loss_weights=class_weight,\n",
    "    multi_label=False,\n",
    ")\n",
    "# Uncomment the below line if you want to run the network using the FocalLoss as Loss function.\n",
    "# classifier.loss_fuction = FocalLoss(alpha=0.4, gamma=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Model training  with basic parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "training_models_path = os.path.join(FLAIR_OUTPUT_DIR, \"flair_training_models\")\n",
    "trainer = ModelTrainer(classifier, corpus)\n",
    "## Failed attempt to use WeightedRandomSampler to the\n",
    "# trainer.train(\n",
    "#     FLAIR_OUTPUT_DIR,\n",
    "#     learning_rate=0.1,\n",
    "#     mini_batch_size=64,\n",
    "#     patience=5,\n",
    "#     max_epochs=20,\n",
    "#     embeddings_storage_mode=\"gpu\",\n",
    "#     sampler=WeightedRandomSampler(\n",
    "#     list(class_weight.values()), len(class_weight.keys())\n",
    "#     ),\n",
    "# )\n",
    "trainer.train(\n",
    "    training_models_path,\n",
    "    learning_rate=0.1,\n",
    "    mini_batch_size=64,\n",
    "    patience=5,\n",
    "    max_epochs=10,\n",
    "    embeddings_storage_mode=\"gpu\",\n",
    "    #     sampler=WeightedRandomSampler(\n",
    "    #         list(class_weight.values()), len(class_weight.keys())\n",
    "    #     ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-23 00:17:38,174 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 00:17:38,192 Model: \"TextClassifier(\n",
      "  (document_embeddings): DocumentRNNEmbeddings(\n",
      "    (embeddings): StackedEmbeddings(\n",
      "      (list_embedding_0): WordEmbeddings('/home/giannhs/PycharmProjects/data_challenge/data/flair_emdg_dir/twitter.gensim')\n",
      "      (list_embedding_1): WordEmbeddings('/home/giannhs/PycharmProjects/data_challenge/data/flair_emdg_dir/glove.gensim')\n",
      "      (list_embedding_2): FlairEmbeddings(\n",
      "        (lm): LanguageModel(\n",
      "          (drop): Dropout(p=0.25, inplace=False)\n",
      "          (encoder): Embedding(275, 100)\n",
      "          (rnn): LSTM(100, 1024)\n",
      "          (decoder): Linear(in_features=1024, out_features=275, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (list_embedding_3): FlairEmbeddings(\n",
      "        (lm): LanguageModel(\n",
      "          (drop): Dropout(p=0.25, inplace=False)\n",
      "          (encoder): Embedding(275, 100)\n",
      "          (rnn): LSTM(100, 1024)\n",
      "          (decoder): Linear(in_features=1024, out_features=275, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (word_reprojection_map): Linear(in_features=2248, out_features=256, bias=True)\n",
      "    (rnn): GRU(256, 512, num_layers=2, batch_first=True, bidirectional=True)\n",
      "    (dropout): Dropout(p=0.2, inplace=False)\n",
      "  )\n",
      "  (decoder): Linear(in_features=2048, out_features=15, bias=True)\n",
      "  (loss_function): CrossEntropyLoss()\n",
      "  (beta): 1.0\n",
      "  (weights): OrderedDict([(b'0', 1.1758), (b'1', 11.5366), (b'2', 2.781), (b'3', 4.7218), (b'4', 6.207), (b'5', 36.725), (b'6', 70.3245), (b'7', 367.25), (b'8', 54.1844), (b'9', 58.5), (b'10', 13.1946), (b'11', 20.4028), (b'12', 22.1087), (b'13', 31.9348), (b'14', 20.2776)])\n",
      "  (weight_tensor) tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      ")\"\n",
      "2020-06-23 00:17:38,193 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 00:17:38,193 Corpus: \"Corpus: 5273 train + 658 dev + 659 test sentences\"\n",
      "2020-06-23 00:17:38,194 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 00:17:38,195 Parameters:\n",
      "2020-06-23 00:17:38,196  - learning_rate: \"0.1\"\n",
      "2020-06-23 00:17:38,198  - mini_batch_size: \"64\"\n",
      "2020-06-23 00:17:38,200  - patience: \"5\"\n",
      "2020-06-23 00:17:38,201  - anneal_factor: \"0.5\"\n",
      "2020-06-23 00:17:38,203  - max_epochs: \"10\"\n",
      "2020-06-23 00:17:38,204  - shuffle: \"True\"\n",
      "2020-06-23 00:17:38,205  - train_with_dev: \"False\"\n",
      "2020-06-23 00:17:38,207  - batch_growth_annealing: \"False\"\n",
      "2020-06-23 00:17:38,208 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 00:17:38,209 Model training base path: \"/home/giannhs/PycharmProjects/data_challenge/data/flair_output_dir/flair_training_models\"\n",
      "2020-06-23 00:17:38,210 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 00:17:38,211 Device: cpu\n",
      "2020-06-23 00:17:38,212 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 00:17:38,214 Embeddings storage mode: gpu\n",
      "2020-06-23 00:17:38,217 ----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "## Plot training results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot training results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.visual.training_curves import Plotter\n",
    "import os\n",
    "\n",
    "flair_plt = Plotter()\n",
    "training_res_path = os.path.join(FLAIR_OUTPUT_DIR, \"loss.tsv\")\n",
    "print(training_res_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flair_plt.plot_training_curves(training_res_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flair_plt.plot_learning_rate(training_res_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load already trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_models_path = os.path.join(FLAIR_OUTPUT_DIR, \"flair_training_models\")\n",
    "classifier_gru = TextClassifier.load(\n",
    "    os.path.join(training_models_path, \"best-model.pt\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "source": [
    "### Model prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = classifier_gru.predict(list(datasets[\"test\"].post), multi_class_prob=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions[0].annotation_layers[\"class\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_per_class = [\n",
    "    {\n",
    "        f\"class_{class_pred.value}\": class_pred.score\n",
    "        for class_pred in pred.annotation_layers[\"class\"]\n",
    "    }\n",
    "    for pred in predictions\n",
    "]\n",
    "predictions_scores_df = pd.DataFrame.from_dict(score_per_class)\n",
    "predictions_scores_df.index = datasets[\"test\"].post_id\n",
    "predictions_scores_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_scores_df = predictions_scores_df[[f\"class_{i}\" for i in range(15)]]\n",
    "predictions_scores_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_scores_df.to_csv(\n",
    "    os.path.join(FLAIR_PREDICTION_OUTPUTS, \"gru_prediction.csv\"),\n",
    "    index=True,\n",
    "    header=True,\n",
    "    index_label=\"id\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
