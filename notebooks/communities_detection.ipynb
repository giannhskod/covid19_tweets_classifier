{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "The nb_black extension is already loaded. To reload it, use:\n",
      "  %reload_ext nb_black\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 25;\n",
       "                var nbb_unformatted_code = \"import csv\\nimport os\\nimport networkx as nx\\nimport numpy as np\\nfrom sklearn.linear_model import LogisticRegression\\nimport community as community_louvain\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.metrics import log_loss\\nfrom sklearn.pipeline import make_pipeline\\nfrom sklearn.preprocessing import StandardScaler\\nfrom sklearn.svm import SVC\\nfrom sklearn.ensemble import RandomForestClassifier\\nfrom sklearn.linear_model import SGDClassifier\\nimport pickle\\nfrom sklearn.ensemble import VotingClassifier\\n\\nimport parent_modules\\nimport preprocessor\\n\\n%load_ext autoreload\\n%load_ext nb_black\\n%autoreload 2\\n\\nfrom definitions import *\";\n",
       "                var nbb_formatted_code = \"import csv\\nimport os\\nimport networkx as nx\\nimport numpy as np\\nfrom sklearn.linear_model import LogisticRegression\\nimport community as community_louvain\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.metrics import log_loss\\nfrom sklearn.pipeline import make_pipeline\\nfrom sklearn.preprocessing import StandardScaler\\nfrom sklearn.svm import SVC\\nfrom sklearn.ensemble import RandomForestClassifier\\nfrom sklearn.linear_model import SGDClassifier\\nimport pickle\\nfrom sklearn.ensemble import VotingClassifier\\n\\nimport parent_modules\\nimport preprocessor\\n\\n%load_ext autoreload\\n%load_ext nb_black\\n%autoreload 2\\n\\nfrom definitions import *\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import csv\n",
    "import os\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import community as community_louvain\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "import pickle\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "import parent_modules\n",
    "import preprocessor\n",
    "\n",
    "%load_ext autoreload\n",
    "%load_ext nb_black\n",
    "%autoreload 2\n",
    "\n",
    "from definitions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the retweet network as a directed graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 784898\n",
      "Number of edges: 7401920\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 2;\n",
       "                var nbb_unformatted_code = \"G = nx.read_weighted_edgelist(\\n    os.path.join(DATA_DIR, \\\"retweet_weighted.edgelist\\\"),\\n    create_using=nx.DiGraph(),\\n    nodetype=int,\\n)\\n\\nprint(\\\"Number of nodes:\\\", G.number_of_nodes())\\nprint(\\\"Number of edges:\\\", G.number_of_edges())\";\n",
       "                var nbb_formatted_code = \"G = nx.read_weighted_edgelist(\\n    os.path.join(DATA_DIR, \\\"retweet_weighted.edgelist\\\"),\\n    create_using=nx.DiGraph(),\\n    nodetype=int,\\n)\\n\\nprint(\\\"Number of nodes:\\\", G.number_of_nodes())\\nprint(\\\"Number of edges:\\\", G.number_of_edges())\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "G = nx.read_weighted_edgelist(\n",
    "    os.path.join(DATA_DIR, \"retweet_weighted.edgelist\"),\n",
    "    create_using=nx.DiGraph(),\n",
    "    nodetype=int,\n",
    ")\n",
    "\n",
    "print(\"Number of nodes:\", G.number_of_nodes())\n",
    "print(\"Number of edges:\", G.number_of_edges())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store the ID of the user that posted each message, and initialize for each user a 15-dimensional vector that will store the number of messages of each class posted by the user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 3;\n",
       "                var nbb_unformatted_code = \"posted_by = dict()\\nposts_per_class = dict()\\nusers = list()\\nwith open(os.path.join(DATA_DIR, \\\"posts.tsv\\\"), \\\"r\\\") as f:\\n    for line in f:\\n        t = line.split(\\\"\\\\t\\\")\\n        posted_by[int(t[0])] = int(t[1])\\n        posts_per_class[int(t[1])] = np.zeros(15)\\n        users.append(int(t[1]))\\nusers = set(users)\";\n",
       "                var nbb_formatted_code = \"posted_by = dict()\\nposts_per_class = dict()\\nusers = list()\\nwith open(os.path.join(DATA_DIR, \\\"posts.tsv\\\"), \\\"r\\\") as f:\\n    for line in f:\\n        t = line.split(\\\"\\\\t\\\")\\n        posted_by[int(t[0])] = int(t[1])\\n        posts_per_class[int(t[1])] = np.zeros(15)\\n        users.append(int(t[1]))\\nusers = set(users)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "posted_by = dict()\n",
    "posts_per_class = dict()\n",
    "users = list()\n",
    "with open(os.path.join(DATA_DIR, \"posts.tsv\"), \"r\") as f:\n",
    "    for line in f:\n",
    "        t = line.split(\"\\t\")\n",
    "        posted_by[int(t[0])] = int(t[1])\n",
    "        posts_per_class[int(t[1])] = np.zeros(15)\n",
    "        users.append(int(t[1]))\n",
    "users = set(users)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read training data. Given a message posted by user A that belongs to class B, increase the number of posts of class B posted by user A by 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 4;\n",
       "                var nbb_unformatted_code = \"train_index = list()\\ny_train = list()\\nwith open(os.path.join(DATA_DIR, \\\"train.csv\\\"), \\\"r\\\") as f:\\n    for line in f:\\n        t = line.split(\\\",\\\")\\n        train_index.append(int(t[0]))\\n        y_train.append(int(t[1]))\\n        posts_per_class[posted_by[int(t[0])]][int(t[1][:-1])] += 1\";\n",
       "                var nbb_formatted_code = \"train_index = list()\\ny_train = list()\\nwith open(os.path.join(DATA_DIR, \\\"train.csv\\\"), \\\"r\\\") as f:\\n    for line in f:\\n        t = line.split(\\\",\\\")\\n        train_index.append(int(t[0]))\\n        y_train.append(int(t[1]))\\n        posts_per_class[posted_by[int(t[0])]][int(t[1][:-1])] += 1\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_index = list()\n",
    "y_train = list()\n",
    "with open(os.path.join(DATA_DIR, \"train.csv\"), \"r\") as f:\n",
    "    for line in f:\n",
    "        t = line.split(\",\")\n",
    "        train_index.append(int(t[0]))\n",
    "        y_train.append(int(t[1]))\n",
    "        posts_per_class[posted_by[int(t[0])]][int(t[1][:-1])] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 5;\n",
       "                var nbb_unformatted_code = \"test_index = list()\\nwith open(os.path.join(DATA_DIR, \\\"test.csv\\\"), \\\"r\\\") as f:\\n    for line in f:\\n        t = line.split(\\\",\\\")\\n        test_index.append(int(t[0]))\";\n",
       "                var nbb_formatted_code = \"test_index = list()\\nwith open(os.path.join(DATA_DIR, \\\"test.csv\\\"), \\\"r\\\") as f:\\n    for line in f:\\n        t = line.split(\\\",\\\")\\n        test_index.append(int(t[0]))\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_index = list()\n",
    "with open(os.path.join(DATA_DIR, \"test.csv\"), \"r\") as f:\n",
    "    for line in f:\n",
    "        t = line.split(\",\")\n",
    "        test_index.append(int(t[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Community Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 6;\n",
       "                var nbb_unformatted_code = \"G_un = G.copy().to_undirected()\\nfor node in G:\\n    for ngbr in nx.neighbors(G, node):\\n        if node in nx.neighbors(G, ngbr):\\n            G_un.edges[node, ngbr][\\\"weight\\\"] = (\\n                G.edges[node, ngbr][\\\"weight\\\"] + G.edges[ngbr, node][\\\"weight\\\"]\\n            )\";\n",
       "                var nbb_formatted_code = \"G_un = G.copy().to_undirected()\\nfor node in G:\\n    for ngbr in nx.neighbors(G, node):\\n        if node in nx.neighbors(G, ngbr):\\n            G_un.edges[node, ngbr][\\\"weight\\\"] = (\\n                G.edges[node, ngbr][\\\"weight\\\"] + G.edges[ngbr, node][\\\"weight\\\"]\\n            )\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "G_un = G.copy().to_undirected()\n",
    "for node in G:\n",
    "    for ngbr in nx.neighbors(G, node):\n",
    "        if node in nx.neighbors(G, ngbr):\n",
    "            G_un.edges[node, ngbr][\"weight\"] = (\n",
    "                G.edges[node, ngbr][\"weight\"] + G.edges[ngbr, node][\"weight\"]\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of edges: 7401920\n",
      "Number of undirected edges: 7383985\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 7;\n",
       "                var nbb_unformatted_code = \"print(\\\"Number of edges:\\\", G.number_of_edges())\\nprint(\\\"Number of undirected edges:\\\", G_un.number_of_edges())\";\n",
       "                var nbb_formatted_code = \"print(\\\"Number of edges:\\\", G.number_of_edges())\\nprint(\\\"Number of undirected edges:\\\", G_un.number_of_edges())\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Number of edges:\", G.number_of_edges())\n",
    "print(\"Number of undirected edges:\", G_un.number_of_edges())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 8;\n",
       "                var nbb_unformatted_code = \"initial_partition = dict()\\nfor user in G.nodes():\\n    initial_partition[user] = -1\\n    has_succ = False\\n    has_pred = False\\n    succ_posts_per_class = np.zeros((1, 15))\\n    pred_posts_per_class = np.zeros((1, 15))\\n    for successor in G.successors(user):\\n        if successor in posts_per_class:\\n            has_succ = True\\n            succ_posts_per_class[0,:15] += posts_per_class[successor]\\n    for predecessor in G.predecessors(user):\\n        if predecessor in posts_per_class:\\n            has_pred = True\\n            pred_posts_per_class[0,:15] += posts_per_class[predecessor]\\n\\n    if user in posts_per_class:\\n        initial_partition[user] = posts_per_class[user].tolist().index(max(posts_per_class[user]))\\n    elif has_pred:\\n        maxValue = np.max(pred_posts_per_class)\\n        index_of_maximum = (np.where(pred_posts_per_class[0] == maxValue)[0])[0]\\n        initial_partition[user] = index_of_maximum+15\\n    elif has_succ:\\n        maxValue = np.max(succ_posts_per_class)\\n        index_of_maximum = (np.where(succ_posts_per_class[0] == maxValue)[0])[0]\\n        initial_partition[user] = index_of_maximum+30\";\n",
       "                var nbb_formatted_code = \"initial_partition = dict()\\nfor user in G.nodes():\\n    initial_partition[user] = -1\\n    has_succ = False\\n    has_pred = False\\n    succ_posts_per_class = np.zeros((1, 15))\\n    pred_posts_per_class = np.zeros((1, 15))\\n    for successor in G.successors(user):\\n        if successor in posts_per_class:\\n            has_succ = True\\n            succ_posts_per_class[0, :15] += posts_per_class[successor]\\n    for predecessor in G.predecessors(user):\\n        if predecessor in posts_per_class:\\n            has_pred = True\\n            pred_posts_per_class[0, :15] += posts_per_class[predecessor]\\n\\n    if user in posts_per_class:\\n        initial_partition[user] = (\\n            posts_per_class[user].tolist().index(max(posts_per_class[user]))\\n        )\\n    elif has_pred:\\n        maxValue = np.max(pred_posts_per_class)\\n        index_of_maximum = (np.where(pred_posts_per_class[0] == maxValue)[0])[0]\\n        initial_partition[user] = index_of_maximum + 15\\n    elif has_succ:\\n        maxValue = np.max(succ_posts_per_class)\\n        index_of_maximum = (np.where(succ_posts_per_class[0] == maxValue)[0])[0]\\n        initial_partition[user] = index_of_maximum + 30\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "initial_partition = dict()\n",
    "for user in G.nodes():\n",
    "    initial_partition[user] = -1\n",
    "    has_succ = False\n",
    "    has_pred = False\n",
    "    succ_posts_per_class = np.zeros((1, 15))\n",
    "    pred_posts_per_class = np.zeros((1, 15))\n",
    "    for successor in G.successors(user):\n",
    "        if successor in posts_per_class:\n",
    "            has_succ = True\n",
    "            succ_posts_per_class[0,:15] += posts_per_class[successor]\n",
    "    for predecessor in G.predecessors(user):\n",
    "        if predecessor in posts_per_class:\n",
    "            has_pred = True\n",
    "            pred_posts_per_class[0,:15] += posts_per_class[predecessor]\n",
    "\n",
    "    if user in posts_per_class:\n",
    "        initial_partition[user] = posts_per_class[user].tolist().index(max(posts_per_class[user]))\n",
    "    elif has_pred:\n",
    "        maxValue = np.max(pred_posts_per_class)\n",
    "        index_of_maximum = (np.where(pred_posts_per_class[0] == maxValue)[0])[0]\n",
    "        initial_partition[user] = index_of_maximum+15\n",
    "    elif has_succ:\n",
    "        maxValue = np.max(succ_posts_per_class)\n",
    "        index_of_maximum = (np.where(succ_posts_per_class[0] == maxValue)[0])[0]\n",
    "        initial_partition[user] = index_of_maximum+30\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 has occurred 6994 times\n",
      "1 has occurred 458 times\n",
      "2 has occurred 1855 times\n",
      "3 has occurred 811 times\n",
      "4 has occurred 829 times\n",
      "5 has occurred 105 times\n",
      "6 has occurred 78 times\n",
      "7 has occurred 14 times\n",
      "8 has occurred 75 times\n",
      "9 has occurred 94 times\n",
      "10 has occurred 416 times\n",
      "11 has occurred 230 times\n",
      "12 has occurred 246 times\n",
      "13 has occurred 143 times\n",
      "14 has occurred 292 times\n",
      "15 has occurred 13780 times\n",
      "16 has occurred 2293 times\n",
      "17 has occurred 8314 times\n",
      "18 has occurred 1070 times\n",
      "19 has occurred 2743 times\n",
      "20 has occurred 209 times\n",
      "21 has occurred 383 times\n",
      "22 has occurred 26 times\n",
      "23 has occurred 104 times\n",
      "24 has occurred 503 times\n",
      "25 has occurred 1249 times\n",
      "26 has occurred 795 times\n",
      "27 has occurred 935 times\n",
      "28 has occurred 211 times\n",
      "29 has occurred 420 times\n",
      "30 has occurred 354732 times\n",
      "31 has occurred 71960 times\n",
      "32 has occurred 132642 times\n",
      "33 has occurred 101924 times\n",
      "34 has occurred 22967 times\n",
      "35 has occurred 3978 times\n",
      "36 has occurred 4476 times\n",
      "37 has occurred 264 times\n",
      "38 has occurred 2078 times\n",
      "39 has occurred 2248 times\n",
      "40 has occurred 14044 times\n",
      "41 has occurred 18213 times\n",
      "42 has occurred 5336 times\n",
      "43 has occurred 957 times\n",
      "44 has occurred 3404 times\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 9;\n",
       "                var nbb_unformatted_code = \"for comm in set(initial_partition.values()):\\n    times = list(initial_partition.values()).count(comm)\\n    print(\\\"{} has occurred {} times\\\".format(comm, times))\";\n",
       "                var nbb_formatted_code = \"for comm in set(initial_partition.values()):\\n    times = list(initial_partition.values()).count(comm)\\n    print(\\\"{} has occurred {} times\\\".format(comm, times))\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for comm in set(initial_partition.values()):\n",
    "    times = list(initial_partition.values()).count(comm)\n",
    "    print(\"{} has occurred {} times\".format(comm, times))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Create the training matrix. Each row corresponds to a message. Use the following 15-dimensional vector of the user that posted the message and concatenate to that vector the following three features:<br/>(1) in-degree of user <br/> (2) out-degree of user <br/> (3) community user belongs to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 10;\n",
       "                var nbb_unformatted_code = \"X_train = np.zeros((len(train_index), 16))\\nfor i, idx in enumerate(train_index):\\n    for successor in G.successors(posted_by[idx]):\\n        if successor in posts_per_class:\\n            X_train[i, :15] += posts_per_class[successor]\\n\\n    for predecessor in G.predecessors(posted_by[idx]):\\n        if predecessor in posts_per_class:\\n            X_train[i, :15] += posts_per_class[predecessor]\\n    if np.sum(X_train[i, :15]) > 0:\\n        X_train[i, :15] /= np.sum(X_train[i, :15])\\n\\n    X_train[i, 15] = G.degree(posted_by[idx])\";\n",
       "                var nbb_formatted_code = \"X_train = np.zeros((len(train_index), 16))\\nfor i, idx in enumerate(train_index):\\n    for successor in G.successors(posted_by[idx]):\\n        if successor in posts_per_class:\\n            X_train[i, :15] += posts_per_class[successor]\\n\\n    for predecessor in G.predecessors(posted_by[idx]):\\n        if predecessor in posts_per_class:\\n            X_train[i, :15] += posts_per_class[predecessor]\\n    if np.sum(X_train[i, :15]) > 0:\\n        X_train[i, :15] /= np.sum(X_train[i, :15])\\n\\n    X_train[i, 15] = G.degree(posted_by[idx])\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_train = np.zeros((len(train_index), 16))\n",
    "for i, idx in enumerate(train_index):\n",
    "    for successor in G.successors(posted_by[idx]):\n",
    "        if successor in posts_per_class:\n",
    "            X_train[i, :15] += posts_per_class[successor]\n",
    "\n",
    "    for predecessor in G.predecessors(posted_by[idx]):\n",
    "        if predecessor in posts_per_class:\n",
    "            X_train[i, :15] += posts_per_class[predecessor]\n",
    "    if np.sum(X_train[i, :15]) > 0:\n",
    "        X_train[i, :15] /= np.sum(X_train[i, :15])\n",
    "\n",
    "    X_train[i, 15] = G.degree(posted_by[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the test matrix. Each row corresponds to a message.Use the following 15-dimensional vector of the user that posted the message and concatenate to that vector the following three features: <br/>(1) in-degree of user<br/>(2) out-degree of user <br/> (3) community user belongs to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 11;\n",
       "                var nbb_unformatted_code = \"X_test = np.zeros((len(test_index), 16))\\nfor i, idx in enumerate(test_index):\\n    for successor in G.successors(posted_by[idx]):\\n        if successor in posts_per_class:\\n            X_test[i, :15] += posts_per_class[successor]\\n\\n    for predecessor in G.predecessors(posted_by[idx]):\\n        if predecessor in posts_per_class:\\n            X_test[i, :15] += posts_per_class[predecessor]\\n\\n    if np.sum(X_test[i, :15]) > 0:\\n        X_test[i, :15] /= np.sum(X_test[i, :15])\\n\\n    X_test[i, 15] = G.degree(posted_by[idx])\";\n",
       "                var nbb_formatted_code = \"X_test = np.zeros((len(test_index), 16))\\nfor i, idx in enumerate(test_index):\\n    for successor in G.successors(posted_by[idx]):\\n        if successor in posts_per_class:\\n            X_test[i, :15] += posts_per_class[successor]\\n\\n    for predecessor in G.predecessors(posted_by[idx]):\\n        if predecessor in posts_per_class:\\n            X_test[i, :15] += posts_per_class[predecessor]\\n\\n    if np.sum(X_test[i, :15]) > 0:\\n        X_test[i, :15] /= np.sum(X_test[i, :15])\\n\\n    X_test[i, 15] = G.degree(posted_by[idx])\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_test = np.zeros((len(test_index), 16))\n",
    "for i, idx in enumerate(test_index):\n",
    "    for successor in G.successors(posted_by[idx]):\n",
    "        if successor in posts_per_class:\n",
    "            X_test[i, :15] += posts_per_class[successor]\n",
    "\n",
    "    for predecessor in G.predecessors(posted_by[idx]):\n",
    "        if predecessor in posts_per_class:\n",
    "            X_test[i, :15] += posts_per_class[predecessor]\n",
    "\n",
    "    if np.sum(X_test[i, :15]) > 0:\n",
    "        X_test[i, :15] /= np.sum(X_test[i, :15])\n",
    "\n",
    "    X_test[i, 15] = G.degree(posted_by[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 26;\n",
       "                var nbb_unformatted_code = \"def run_community_louvain_detection(\\n    Graph_un, initial, train_idx, posters, test_idx, load_pickle=True\\n):\\n    def recalculate_louvain():\\n        # compute the best partition\\n        low_partition = community_louvain.best_partition(Graph_un, resolution=0.8)\\n        low_res_train = np.zeros((len(train_idx), 2))\\n        for i, idx in enumerate(train_idx):\\n            low_res_train[i, 0] = idx\\n            low_res_train[i, 1] = low_partition[posters[idx]]\\n\\n        low_res_test = np.zeros((len(test_idx), 2))\\n        for i, idx in enumerate(test_idx):\\n            low_res_test[i, 0] = idx\\n            low_res_test[i, 1] = low_partition[posters[idx]]\\n        low_res = {\\\"low_res_train\\\": low_res_train, \\\"low_res_test\\\": low_res_test}\\n\\n        # compute the best partition\\n        high_partition = community_louvain.best_partition(Graph_un, resolution=3)\\n        high_res_train = np.zeros((len(train_idx), 2))\\n        for i, idx in enumerate(train_idx):\\n            high_res_train[i, 0] = idx\\n            high_res_train[i, 1] = high_partition[posters[idx]]\\n\\n        high_res_test = np.zeros((len(test_idx), 2))\\n        for i, idx in enumerate(test_idx):\\n            high_res_test[i, 0] = idx\\n            high_res_test[i, 1] = high_partition[posters[idx]]\\n        high_res = {\\\"high_res_train\\\": high_res_train, \\\"high_res_test\\\": high_res_test}\\n\\n        # compute the best partition\\n        initial = community_louvain.best_partition(Graph_un, partition=initial)\\n        initial_train = np.zeros((len(train_idx), 2))\\n        for i, idx in enumerate(train_idx):\\n            initial_train[i, 0] = idx\\n            initial_train[i, 1] = initial[posters[idx]]\\n\\n        initial_test = np.zeros((len(test_idx), 2))\\n        for i, idx in enumerate(test_idx):\\n            initial_test[i, 0] = idx\\n            initial_test[i, 1] = initial[posters[idx]]\\n\\n        return {\\\"low_res\\\": low_res, \\\"high_res\\\": high_res, \\\"initial\\\": initial}\\n\\n    community_pickle = os.path.join(ML_CLASSIFIERS_PICKLE_DIR, \\\"communities.pickle\\\")\\n\\n    if load_pickle:\\n        try:\\n            with (open(community_pickle, \\\"rb\\\")) as openfile:\\n                dict_to_export = pickle.load(openfile)\\n\\n        except Exception as e:\\n            dict_to_export = recalculate_louvain()\\n            pickle.dump(dict_to_export, handle, protocol=pickle.HIGHEST_PROTOCOL)\\n    else:\\n        dict_to_export = recalculate_louvain()\\n\\n    return dict_to_export\";\n",
       "                var nbb_formatted_code = \"def run_community_louvain_detection(\\n    Graph_un, initial, train_idx, posters, test_idx, load_pickle=True\\n):\\n    def recalculate_louvain():\\n        # compute the best partition\\n        low_partition = community_louvain.best_partition(Graph_un, resolution=0.8)\\n        low_res_train = np.zeros((len(train_idx), 2))\\n        for i, idx in enumerate(train_idx):\\n            low_res_train[i, 0] = idx\\n            low_res_train[i, 1] = low_partition[posters[idx]]\\n\\n        low_res_test = np.zeros((len(test_idx), 2))\\n        for i, idx in enumerate(test_idx):\\n            low_res_test[i, 0] = idx\\n            low_res_test[i, 1] = low_partition[posters[idx]]\\n        low_res = {\\\"low_res_train\\\": low_res_train, \\\"low_res_test\\\": low_res_test}\\n\\n        # compute the best partition\\n        high_partition = community_louvain.best_partition(Graph_un, resolution=3)\\n        high_res_train = np.zeros((len(train_idx), 2))\\n        for i, idx in enumerate(train_idx):\\n            high_res_train[i, 0] = idx\\n            high_res_train[i, 1] = high_partition[posters[idx]]\\n\\n        high_res_test = np.zeros((len(test_idx), 2))\\n        for i, idx in enumerate(test_idx):\\n            high_res_test[i, 0] = idx\\n            high_res_test[i, 1] = high_partition[posters[idx]]\\n        high_res = {\\\"high_res_train\\\": high_res_train, \\\"high_res_test\\\": high_res_test}\\n\\n        # compute the best partition\\n        initial = community_louvain.best_partition(Graph_un, partition=initial)\\n        initial_train = np.zeros((len(train_idx), 2))\\n        for i, idx in enumerate(train_idx):\\n            initial_train[i, 0] = idx\\n            initial_train[i, 1] = initial[posters[idx]]\\n\\n        initial_test = np.zeros((len(test_idx), 2))\\n        for i, idx in enumerate(test_idx):\\n            initial_test[i, 0] = idx\\n            initial_test[i, 1] = initial[posters[idx]]\\n\\n        return {\\\"low_res\\\": low_res, \\\"high_res\\\": high_res, \\\"initial\\\": initial}\\n\\n    community_pickle = os.path.join(ML_CLASSIFIERS_PICKLE_DIR, \\\"communities.pickle\\\")\\n\\n    if load_pickle:\\n        try:\\n            with (open(community_pickle, \\\"rb\\\")) as openfile:\\n                dict_to_export = pickle.load(openfile)\\n\\n        except Exception as e:\\n            dict_to_export = recalculate_louvain()\\n            pickle.dump(dict_to_export, handle, protocol=pickle.HIGHEST_PROTOCOL)\\n    else:\\n        dict_to_export = recalculate_louvain()\\n\\n    return dict_to_export\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def run_community_louvain_detection(\n",
    "    Graph_un, initial, train_idx, posters, test_idx, load_pickle=True\n",
    "):\n",
    "    def recalculate_louvain():\n",
    "        # compute the best partition\n",
    "        low_partition = community_louvain.best_partition(Graph_un, resolution=0.8)\n",
    "        low_res_train = np.zeros((len(train_idx), 2))\n",
    "        for i, idx in enumerate(train_idx):\n",
    "            low_res_train[i, 0] = idx\n",
    "            low_res_train[i, 1] = low_partition[posters[idx]]\n",
    "\n",
    "        low_res_test = np.zeros((len(test_idx), 2))\n",
    "        for i, idx in enumerate(test_idx):\n",
    "            low_res_test[i, 0] = idx\n",
    "            low_res_test[i, 1] = low_partition[posters[idx]]\n",
    "        low_res = {\"low_res_train\": low_res_train, \"low_res_test\": low_res_test}\n",
    "\n",
    "        # compute the best partition\n",
    "        high_partition = community_louvain.best_partition(Graph_un, resolution=3)\n",
    "        high_res_train = np.zeros((len(train_idx), 2))\n",
    "        for i, idx in enumerate(train_idx):\n",
    "            high_res_train[i, 0] = idx\n",
    "            high_res_train[i, 1] = high_partition[posters[idx]]\n",
    "\n",
    "        high_res_test = np.zeros((len(test_idx), 2))\n",
    "        for i, idx in enumerate(test_idx):\n",
    "            high_res_test[i, 0] = idx\n",
    "            high_res_test[i, 1] = high_partition[posters[idx]]\n",
    "        high_res = {\"high_res_train\": high_res_train, \"high_res_test\": high_res_test}\n",
    "\n",
    "        # compute the best partition\n",
    "        first_part = community_louvain.best_partition(Graph_un, partition=initial)\n",
    "        initial_train = np.zeros((len(train_idx), 2))\n",
    "        for i, idx in enumerate(train_idx):\n",
    "            initial_train[i, 0] = idx\n",
    "            initial_train[i, 1] = initial[posters[idx]]\n",
    "\n",
    "        initial_test = np.zeros((len(test_idx), 2))\n",
    "        for i, idx in enumerate(test_idx):\n",
    "            initial_test[i, 0] = idx\n",
    "            initial_test[i, 1] = initial[posters[idx]]\n",
    "\n",
    "        return {\"low_res\": low_res, \"high_res\": high_res, \"initial\": first_part}\n",
    "\n",
    "    community_pickle = os.path.join(ML_CLASSIFIERS_PICKLE_DIR, \"communities.pickle\")\n",
    "\n",
    "    if load_pickle:\n",
    "        try:\n",
    "            with (open(community_pickle, \"rb\")) as openfile:\n",
    "                dict_to_export = pickle.load(openfile)\n",
    "\n",
    "        except Exception as e:\n",
    "            dict_to_export = recalculate_louvain()\n",
    "            pickle.dump(dict_to_export, community_pickle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    else:\n",
    "        dict_to_export = recalculate_louvain()\n",
    "\n",
    "    return dict_to_export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 22;\n",
       "                var nbb_unformatted_code = \"y_train_pred_lin = list()\\ny_pred_lin = list()\\n\\nwith open(\\\"text_train_predictions.csv\\\", \\\"r\\\") as f:\\n    for line in f:\\n        t = line.split(\\\",\\\")\\n        t = [float(i) for i in t]\\n        y_train_pred_lin.append(t)\\n\\nwith open(\\\"text_predictions.csv\\\", \\\"r\\\") as f:\\n    for line in f:\\n        t = line.split(\\\",\\\")\\n        t = [float(i) for i in t]\\n        y_pred_lin.append(t)\";\n",
       "                var nbb_formatted_code = \"y_train_pred_lin = list()\\ny_pred_lin = list()\\n\\nwith open(\\\"text_train_predictions.csv\\\", \\\"r\\\") as f:\\n    for line in f:\\n        t = line.split(\\\",\\\")\\n        t = [float(i) for i in t]\\n        y_train_pred_lin.append(t)\\n\\nwith open(\\\"text_predictions.csv\\\", \\\"r\\\") as f:\\n    for line in f:\\n        t = line.split(\\\",\\\")\\n        t = [float(i) for i in t]\\n        y_pred_lin.append(t)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_train_pred_lin = list()\n",
    "y_pred_lin = list()\n",
    "\n",
    "with open(\"text_train_predictions.csv\", \"r\") as f:\n",
    "    for line in f:\n",
    "        t = line.split(\",\")\n",
    "        t = [float(i) for i in t]\n",
    "        y_train_pred_lin.append(t)\n",
    "\n",
    "with open(\"text_predictions.csv\", \"r\") as f:\n",
    "    for line in f:\n",
    "        t = line.split(\",\")\n",
    "        t = [float(i) for i in t]\n",
    "        y_pred_lin.append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'initial' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-ab5e7dca2a81>\u001b[0m in \u001b[0;36mrun_community_louvain_detection\u001b[0;34m(Graph_un, initial, train_idx, posters, test_idx, load_pickle)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommunity_pickle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopenfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m                 \u001b[0mdict_to_export\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopenfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/giannhs/PycharmProjects/data_challenge/data/ml_classifiers_pickle_dir/communities.pickle'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-e4d3b98fd491>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m dict_to_export = run_community_louvain_detection(\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mG_un\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_partition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposted_by\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_pickle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m )\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mlow_res\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict_to_export\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"low_res\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-26-ab5e7dca2a81>\u001b[0m in \u001b[0;36mrun_community_louvain_detection\u001b[0;34m(Graph_un, initial, train_idx, posters, test_idx, load_pickle)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m             \u001b[0mdict_to_export\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrecalculate_louvain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m             \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict_to_export\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHIGHEST_PROTOCOL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-26-ab5e7dca2a81>\u001b[0m in \u001b[0;36mrecalculate_louvain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;31m# compute the best partition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0minitial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcommunity_louvain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_partition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGraph_un\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartition\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0minitial_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'initial' referenced before assignment"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 27;\n",
       "                var nbb_unformatted_code = \"dict_to_export = run_community_louvain_detection(\\n    G_un, initial_partition, train_index, posted_by, test_index, load_pickle=True\\n)\\n\\nlow_res = dict_to_export[\\\"low_res\\\"]\\nlow_res_train = low_res[\\\"low_res_train\\\"]\\nlow_res_test = low_res[\\\"low_res_test\\\"]\\n\\nhigh_res = dict_to_export[\\\"high_res\\\"]\\nhigh_res_train = high_res[\\\"high_res_train\\\"]\\nhigh_res_test = high_res[\\\"high_res_test\\\"]\\n\\ninitial = dict_to_export[\\\"initial\\\"]\\ninitial_train = initial[\\\"initial_train\\\"]\\ninitial_test = initial[\\\"initial_test\\\"]\";\n",
       "                var nbb_formatted_code = \"dict_to_export = run_community_louvain_detection(\\n    G_un, initial_partition, train_index, posted_by, test_index, load_pickle=True\\n)\\n\\nlow_res = dict_to_export[\\\"low_res\\\"]\\nlow_res_train = low_res[\\\"low_res_train\\\"]\\nlow_res_test = low_res[\\\"low_res_test\\\"]\\n\\nhigh_res = dict_to_export[\\\"high_res\\\"]\\nhigh_res_train = high_res[\\\"high_res_train\\\"]\\nhigh_res_test = high_res[\\\"high_res_test\\\"]\\n\\ninitial = dict_to_export[\\\"initial\\\"]\\ninitial_train = initial[\\\"initial_train\\\"]\\ninitial_test = initial[\\\"initial_test\\\"]\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dict_to_export = run_community_louvain_detection(\n",
    "    G_un, initial_partition, train_index, posted_by, test_index, load_pickle=True\n",
    ")\n",
    "\n",
    "low_res = dict_to_export[\"low_res\"]\n",
    "low_res_train = low_res[\"low_res_train\"]\n",
    "low_res_test = low_res[\"low_res_test\"]\n",
    "\n",
    "high_res = dict_to_export[\"high_res\"]\n",
    "high_res_train = high_res[\"high_res_train\"]\n",
    "high_res_test = high_res[\"high_res_test\"]\n",
    "\n",
    "initial = dict_to_export[\"initial\"]\n",
    "initial_train = initial[\"initial_train\"]\n",
    "initial_test = initial[\"initial_test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 20;\n",
       "                var nbb_unformatted_code = \"clf_lin = LogisticRegression(\\n    solver=\\\"newton-cg\\\",\\n    multi_class=\\\"multinomial\\\",\\n    class_weight=class_weight,\\n    max_iter = 10000\\n)\\nclf_rd_frst = RandomForestClassifier(\\n    max_depth=150, criterion=\\\"entropy\\\", class_weight=class_weight\\n)\\nmodels = list()\\nmodels.append((\\\"clf_lin\\\", clf_lin))\\nmodels.append((\\\"clf_rd_frst\\\", clf_rd_frst))\\nensemble = VotingClassifier(estimators=models, voting=\\\"soft\\\")\";\n",
       "                var nbb_formatted_code = \"clf_lin = LogisticRegression(\\n    solver=\\\"newton-cg\\\",\\n    multi_class=\\\"multinomial\\\",\\n    class_weight=class_weight,\\n    max_iter=10000,\\n)\\nclf_rd_frst = RandomForestClassifier(\\n    max_depth=150, criterion=\\\"entropy\\\", class_weight=class_weight\\n)\\nmodels = list()\\nmodels.append((\\\"clf_lin\\\", clf_lin))\\nmodels.append((\\\"clf_rd_frst\\\", clf_rd_frst))\\nensemble = VotingClassifier(estimators=models, voting=\\\"soft\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "neighbour_weight = 0.2\n",
    "prediction_weight = 1 - neighbour_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evalutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log loss lin: 0.08614925959452087\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 21;\n",
       "                var nbb_unformatted_code = \"dev_ensemble = ensemble.fit(X_train_dev, y_train_dev)\\nvot_pred_dev = ensemble.predict_proba(X_test_dev)\\nlog = log_loss(y_test_dev, vot_pred_dev)\\nprint(\\\"log loss lin:\\\", log)\";\n",
       "                var nbb_formatted_code = \"dev_ensemble = ensemble.fit(X_train_dev, y_train_dev)\\nvot_pred_dev = ensemble.predict_proba(X_test_dev)\\nlog = log_loss(y_test_dev, vot_pred_dev)\\nprint(\\\"log loss lin:\\\", log)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dev_ensemble = ensemble.fit(X_train_dev, y_train_dev)\n",
    "vot_pred_dev = ensemble.predict_proba(X_test_dev)\n",
    "log = log_loss(y_test_dev, vot_pred_dev)\n",
    "print(\"log loss lin:\", log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 25;\n",
       "                var nbb_unformatted_code = \"full_ensemble = ensemble.fit(X_train_with_comm, y_train)\\nvot_pred_test = full_ensemble.predict_proba(X_test_with_comm)\";\n",
       "                var nbb_formatted_code = \"full_ensemble = ensemble.fit(X_train_with_comm, y_train)\\nvot_pred_test = full_ensemble.predict_proba(X_test_with_comm)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "full_ensemble = ensemble.fit(X_train_with_comm, y_train)\n",
    "vot_pred_test = full_ensemble.predict_proba(X_test_with_comm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 26;\n",
       "                var nbb_unformatted_code = \"with open(\\\"voting_last_chance.csv\\\", \\\"w\\\") as csvfile:\\n    writer = csv.writer(csvfile, delimiter=\\\",\\\")\\n    lst = [\\\"id\\\"]\\n    for i in range(15):\\n        lst.append(\\\"class_\\\" + str(i))\\n    writer.writerow(lst)\\n    for i, idx in enumerate(test_index):\\n        lst = vot_pred_test[i, :].tolist()\\n        lst.insert(0, idx)\\n        writer.writerow(lst)\";\n",
       "                var nbb_formatted_code = \"with open(\\\"voting_last_chance.csv\\\", \\\"w\\\") as csvfile:\\n    writer = csv.writer(csvfile, delimiter=\\\",\\\")\\n    lst = [\\\"id\\\"]\\n    for i in range(15):\\n        lst.append(\\\"class_\\\" + str(i))\\n    writer.writerow(lst)\\n    for i, idx in enumerate(test_index):\\n        lst = vot_pred_test[i, :].tolist()\\n        lst.insert(0, idx)\\n        writer.writerow(lst)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open(\"voting_last_chance.csv\", \"w\") as csvfile:\n",
    "    writer = csv.writer(csvfile, delimiter=\",\")\n",
    "    lst = [\"id\"]\n",
    "    for i in range(15):\n",
    "        lst.append(\"class_\" + str(i))\n",
    "    writer.writerow(lst)\n",
    "    for i, idx in enumerate(test_index):\n",
    "        lst = vot_pred_test[i, :].tolist()\n",
    "        lst.insert(0, idx)\n",
    "        writer.writerow(lst)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}